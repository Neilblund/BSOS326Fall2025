{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fbb3d1-da93-4b6c-a574-73e09ec2b561",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65352a-adde-48f5-bc2a-d5691291a3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install transformers\n",
    "#%pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115f60f-17a7-4f73-9deb-aabce62a388d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "\n",
    "import os\n",
    "# suppressing some warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a31b6-4671-4bc1-9b90-db7252128f13",
   "metadata": {},
   "source": [
    "## Neural Networks for Text Data\n",
    "\n",
    "\n",
    "Neural networks are extremely flexible, which allows you to use them for all kinds of data. They can also be used for text data to do tasks such as sentiment analysis using supervised learning.\n",
    "\n",
    "Go ahead and run this (it will take a moment to finish) and we'll talk about it in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a44ba-a171-497d-a5b9-ce6ebaadc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "module_url = 'https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2'\n",
    "embedding_model = hub.load(module_url)\n",
    "# check to see if it works - should give return: TensorShape([1, 512])\n",
    "embedding_model([\"This is a text\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458dd74-4ff6-404f-bc0a-b1eebd7a05de",
   "metadata": {},
   "source": [
    "# Reviews\n",
    "\n",
    "We'll start by reloading the IMDB movie review corpus that we used a couple of weeks ago. Just to refresh your memory: this is a subset from a larger corpus of user generated IMDB reviews. The dataset contains the full text of each review, along with a numeric label that is equal to 0 if the review was negative and 1 if the review is positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fa26c-1324-45fb-a23b-a2a88131f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dddb5-9a07-40bc-91ec-6236df315551",
   "metadata": {},
   "source": [
    "Just as in previous classes, we're going to be evalauting a model here by creating separate training and testing datasets. We'll also convert these datasets to tensors in order to make it easier to work with them in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1f887-546e-4bb2-b7b8-243cd1d53dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                     reviews['text'],\n",
    "                                     reviews['label'],\n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! \n",
    "print(f\"Training entries: {len(X_train)}, test entries: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cac3d0-f5ec-49ff-8cbe-8d8c1995e064",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In a previous class, we trained a naive bayes classifier to distinguish positive from negative IMDB reviews with a fairly high degree (~84%) accuracy. Now, we're going to try to do the same task using a neural network with an embedding model instead. In the most general sense, an embedding is a low-dimensional representation of a higher-dimensional data set. Text embeddings can be thought of as a kind of spatial model for text, where words or sentences with similar meanings appear \"close\" to one another in space.\n",
    "\n",
    "The methods for inferring embeddings will vary from one model to the next, but a typical approach for getting word embeddings is to take a bunch of text and train a neural network model with a single hidden layer to predict a randomly held out word using the surrounding context:\n",
    "\n",
    "\n",
    "<div style='display: block;margin-left: auto;margin-right: auto; width: 50%;'>\n",
    "    <img src='https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-cbow.png' width=\"600\">C-BOW model <a href='https://lilianweng.github.io/posts/2017-10-15-word-embedding/'>(source)</a></img>\n",
    "</div>\n",
    "\n",
    "The weights from the hidden layer in this model are used as the word embeddings. Terms with similar meanings will appear in similar contexts, so they should also end up with similar values in this embedding space. This method does a surprisingly good job at picking up on nuanced semantic relationships between terms, and since the training data consists entirely of regular english language texts, its comparatively easy to train them on a ton of data and then use them for multiple different machine learning tasks. <a href='https://projector.tensorflow.org/'>There's a good visual representation of some word embedding models here</a>. \n",
    "\n",
    "Transformer-based models take this one step further by modeling context and word order. So, we can use a pre-trained sentence embedding model to convert a full sentence, paragraph, or document and spit out a series of numbers that can function as a context-aware representation of the sentence's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64c139-d717-4abb-9f11-87c405a42d8c",
   "metadata": {},
   "source": [
    "The `embed` object we downloaded earlier is a pre-trained transformer model that is built for general-purpose applications. It takes a list of strings as inputs and returns a vector of 512 numbers that represent that sentence's \"location\" in a 512 dimension space. Here's an example of embedding a sentence and then getting the first 10 elements of the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b3988-e3d0-41b7-8092-049b5f55affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding a sentence about catci and looking at the first 10 elements\n",
    "\n",
    "embedding_model([\"The rattail cactus is native to Mexico.\"])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd881165-a97e-45cd-acee-a8e85ee89bc6",
   "metadata": {},
   "source": [
    "Note that we don't really need to do much (or any) pre-processing of the texts here. The model is trained on mostly unprocessed text, so things like stripping punctuation or removing stopwords often does more harm than good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dae75a-04fc-448b-a3b7-6d54a87d301c",
   "metadata": {},
   "source": [
    "To illustrate what why the embedding is useful, we can use a little code from the <a href='https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder'>online documentation</a> that will allow us to visualize the similarities between the embeddings produced by different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453094c-f41f-4ab3-8cca-0ab8f6a3cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embedding_model(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4339248-63d3-471d-9efa-00f74ac80be8",
   "metadata": {},
   "source": [
    "Below are some sentences from different wikipedia entries. The first two are from the entry on *Citizen Kane*, the last two are from entries on cacti. Note that the terms in both groups share very few terms overall, but take a look at their similarities as measured by the inner products of their respective embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42284b-8f99-47d2-9a06-ce25eddccf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_and_plot([\n",
    "    # two sentences from the Wikipedia entry for citizen kane\n",
    "    \"Citizen Kane is often cited as the greatest film ever.\",\n",
    "    \"Hollywood had shown interest in Welles in 1936.\",\n",
    "    # sentences from entries on cacti\n",
    "    \"The rattail cactus is native to Mexico.\",\n",
    "    \"Prickly pears are frequently found around California.\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b453b-8aad-40b3-9810-0bf114481fc7",
   "metadata": {},
   "source": [
    "In essence, text embeddings give us a more flexibile way to represent text that can account for nuanced aspects of meaning and context, so that sentences about the same general idea are \"close\" in the embedding space even if they share none of the exact same terms. Feeding these inputs - instead of a simple bag of words - into a machine learning model, can allow us to make more effective use of the same data. While 512 features may seem like a lot to handle, note that this is far more compact than the term-document-matrices that we were working with before, and it theoretically captures more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e000-fe1b-43a3-a9b0-fc5b4cccd163",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Now, let's fit a model to predict movie reviews that uses the embedding model. We'll use the embedding layer as our input layer and then include two hidden layers and a sigmoid output layer that will return our predicted probability of a review being negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed= hub.KerasLayer(embedding_model, trainable=False, name='sentence_embedder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc22b-1b82-4650-a744-26e3842d2604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    embed,   \n",
    "    layers.Dense(128, activation='relu', name='hidden layer'),\n",
    "    layers.Dense(1, activation='sigmoid', name='output')    \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf1c5c-3195-46d2-a084-18c6162ebe29",
   "metadata": {},
   "source": [
    "After that, we compile our model and then train it for 15 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dbed4-ad1a-48a6-816a-d78b3ecaedbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting to tensor objects (data type used by tensorflow)\n",
    "X_train_tensor = tf.convert_to_tensor(X_train)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test)\n",
    "tf.convert_to_tensor(y_test)\n",
    "history = model.fit(X_train_tensor, \n",
    "                    y_train_tensor,\n",
    "                    epochs=20,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(X_test_tensor, \n",
    "                                     y_test_tensor),\n",
    "                    verbose=1\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0372d",
   "metadata": {},
   "source": [
    "If you're looking at the output above, you should notice that the model is being trained over multiple \"epochs\". Each epoch represents a single pass through the entire data set, and you'll usually see rapid improvement during the first few iterations, followed by a gradual leveling off in accuracy. Since we're also including data for the \"validation_data\" argument we can compare improvements on the training data accuracy against improvement on the test data accuracy.\n",
    "\n",
    "We'll visualize these results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86280c7",
   "metadata": {},
   "source": [
    "When watching the progress of a model like this, you should pay attention to things like big disparities between training and test performance, because this can indicate overfitting. You might also want to watch out for cases where the results were still rapidly improving when the training stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3d89e-993d-45d4-a5cc-81ec4116fce9",
   "metadata": {},
   "source": [
    "Now we can generate some predictions and look at our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c7891-ae48-49d5-abcd-21c19ff63612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions from test data\n",
    "preds = model.predict(test_tensor).flatten()>=.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed930a3-764b-4538-9921-0e5ac509f2de",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"color:red;font-weight:bold\">Question 1 </h2>\n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">Create a confusion matrix and classification report from the predictions and assess the quality of the classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8669f70-3bdd-4db6-82a9-63b104b4b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844f15b-406c-46a4-8cd0-a446dea60eb1",
   "metadata": {},
   "source": [
    "How does this do? Does it outperform the naive bayes classifier? Why might this be? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f80381-6aaa-4942-b0e9-423ad700cdc0",
   "metadata": {},
   "source": [
    "## Changes to the Model\n",
    "\n",
    "We can make changes to the model to add more layers, use more nodes, train it for longer, or even use a different kind of model. This is part of the overall process for finding the model that has the best performance in terms of accuracy. In reality, we would do these steps many, many times, tuning our model so that it is as good as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff26f9-8455-4790-8c34-c1c36d909d4e",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"color:red;font-weight:bold\">Question 2 </h2>\n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">Change something about the model above and compare your results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac44f48-46f2-4f09-b45e-23fb2bfd0980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64d96b7-3c44-40c8-8055-578f21947969",
   "metadata": {},
   "source": [
    "(Some options are: add an additional hidden layer, run the same model for more epochs, add more nodes to one or more of the layers, or add <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout'>dropout</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f940-d869-48a1-ae1b-4def25ab9dc0",
   "metadata": {},
   "source": [
    "### Pre-built models from Hugging Face\n",
    "\n",
    "In reality, the full IMDB reviews corpus is much larger than what we've been using here, so we would also want to use that data in its entirety for a real world application, but since that takes a while to train, we can use a pre-made model that was trained on this data set to get a sense of how well we could do if we did some more fine-tuning.\n",
    "\n",
    "The [Hugging Face Hub](https://huggingface.co/models) has many models that have been pre-trained for you to use. One of the models hosted there is a <a href='https://huggingface.co/aychang/roberta-base-imdb'>sentiment classifer that was trained on the entire IMDB corpus</a>. We can load this model and see how it performs on our held-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bdf53-ad36-41f8-9dbf-d0245d434650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# load pipeline for IMDB classification\n",
    "tiny_bert = pipeline(\"text-classification\", \"arnabdhar/tinybert-imdb\", \n",
    "                    # encoder expects a specific text length, so this allows texts to be truncated\n",
    "                     truncation =True,\n",
    "                     padding =True,\n",
    "                     max_length = 500\n",
    "                    )        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bde1a-df2d-469d-b063-ba36b689708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to list since thats the input format this model uses\n",
    "predicted = tiny_bert(X_test.tolist())\n",
    "\n",
    "# formatting the test labels to match the output from this model\n",
    "y_test_text = [\"POSITIVE\" if i ==1 else \"NEGATIVE\" for i in y_test ]\n",
    "\n",
    "predicted_labels = [i.get('label') for i in predicted]\n",
    "pd.crosstab(predicted_labels, y_test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "print(classification_report(y_test_text, predicted_labels))\n",
    "\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test_text, predicted_labels))#\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test_text, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4422e8-fb46-493f-aba8-b28a0a8d2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, tiny_bert_preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                              target_names=['Negative', 'Positive']))\n",
    "\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(test_labels, tiny_bert_preds))#\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(test_labels, tiny_bert_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684505cd-6bc2-4a6a-8cca-bca2fd36c5b9",
   "metadata": {},
   "source": [
    "## Other Types of Sentiment\n",
    "\n",
    "The nice thing about these models is that they can be optimized for all sorts of other takss. For example, let's take the Distilbert-base-uncased-emotion model. This provides scores for emotions such as joy or anger. Here's an example of getting the emotions expressed in the first 100 rows the the reviews data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b7962-541b-45a0-b596-3603dc1f760f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True, \n",
    "                             truncation=True,\n",
    "                             padding = True,\n",
    "                             max_length = 500, \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35234530-74a5-46a2-9a51-57926bedd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = emotion_classifier(X_test.tolist())\n",
    "result = [{i.get('label'):i.get('score') for i in j} for j in emotions]\n",
    "emotions_frame = pd.DataFrame(result)\n",
    "emotions_frame['review'] = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d9f39-8a99-49d7-961b-a93c0effe63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotions_frame.review[emotions_frame['joy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41d36e-c8a1-41d7-8322-5866ff0c1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_prediction = pd.concat([pd.DataFrame(i) for i in prediction])\n",
    "emotion_prediction.groupby('label').agg({'score':['min','max','median','mean']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d44fd4-7bc8-4a83-9486-747d961b8f78",
   "metadata": {},
   "source": [
    "You can check out some other options on the hugging face <a href='https://huggingface.co/models'>models page.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae6187-0056-4364-8c0c-3cd0de1472d7",
   "metadata": {},
   "source": [
    "# BertTopic\n",
    "\n",
    "[BertTopic](https://maartengr.github.io/BERTopic/api/bertopic.html) is a transformer-based method for topic modeling that works especially well on short texts. You can read more about the algorithm [here](https://maartengr.github.io/BERTopic/algorithm/algorithm.html).\n",
    "\n",
    "You can try it out with the code below, but be warned that it will take a while to run. For the sake of a speedier model, we'll work with just the headlines and first few sentences from the Fox/CNN news articles from a couple weeks ago. The BertTopic package also provides several built-in functions that make it easy to visualize and explore results, some of which are demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853766bf-a905-4768-b218-084619693032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install packages:\n",
    "#%pip install BERTopic\n",
    "#%pip install SentenceTransformer\n",
    "#%pip install UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753860c-c775-4b93-b077-541599415432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERTopic (this is very slow sometimes)\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# Loading the 4,000 articles sampled from CNN and Fox\n",
    "articles = pd.read_csv('https://github.com/Neilblund/APAN/raw/main/news_sample.csv')\n",
    "# doing some truncation and combining here just to make the model run faster\n",
    "docs= [' '.join(sent_tokenize(i + \" \"+ j)[:5]).strip() for i, j in  zip(articles.headline, articles.text)]\n",
    "# embedding the texts (typically the slowest part)\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "# setting the UMAP model (this makes results reproducible)\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=119)\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic(umap_model = umap_model).fit(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5bbc5-2458-444e-8134-b619cbaa0a39",
   "metadata": {},
   "source": [
    "From here, we can try out some of the built-in functions for exploring the results of the topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc8f1c-1605-4d49-aa42-569243f28f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all topics and the top terms associated with each one\n",
    "topic_model.topic_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514aa6d-2f21-421b-bcd9-75c4c2cebc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view representative documents for topic 0\n",
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a2f33-03f6-445d-9099-430b989f661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09567cb-c39c-4833-98b9-51be28311341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap showing textual similarity of each topic \n",
    "topic_model.visualize_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b810d9a-f849-48f0-b320-c494cd9f9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distance between topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667e174-282b-4039-9877-9ed9fb93fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected topics associated with each outlet (normalized by their overall frequency)\n",
    "topics_per_class = topic_model.topics_per_class(first_200, articles.source)\n",
    "topic_model.visualize_topics_per_class(topics_per_class,  topics=[1, 3, 8, 11,12, 13], normalize_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8a9d4-b4f1-431f-ab48-654741396ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents in space \n",
    "topic_model.visualize_documents(articles.headline, embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e68ed-8e91-4abe-9c02-8c17e93a7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms for selected topics\n",
    "topic_model.visualize_barchart([0, 1, 2, 3,11, 10], n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc40ab3-30ca-42e1-a066-cdcef9a7d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view topics over time (this model also allows topics themselves to vary slightly depending on the time period)\n",
    "# see https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html\n",
    "topics_over_time = topic_model.topics_over_time(docs, articles.date, nr_bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e356d-a3ef-4a8a-8dfc-0e703128b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 topics\n",
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=5)\n",
    "# or try this to see specific topics:\n",
    "#topic_model.visualize_topics_over_time(topics_over_time, topics = [0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b2fbe-8a9c-4ad9-8661-698130e0d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topic distributions across individual terms\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs, calculate_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfcbd1-7aa9-4fb5-9387-cba808c2e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the topic-word distribution for the first document:\n",
    "df = topic_model.visualize_approximate_distribution(docs[0], topic_token_distr[0])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

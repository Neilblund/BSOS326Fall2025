{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fbb3d1-da93-4b6c-a574-73e09ec2b561",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65352a-adde-48f5-bc2a-d5691291a3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install keras_nlp\n",
    "%pip install tensorflow_datasets\n",
    "%pip install transformers\n",
    "%pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115f60f-17a7-4f73-9deb-aabce62a388d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09cbf9",
   "metadata": {},
   "source": [
    "We previously used methods like linear and logistic regression for supervised machine learning. You can think of a logistic regression model as a kind of network that takes a weighted sum of the inputs and then applies a non-linear transformation to produce a predicted probability of y = 1. In neural network parlance, you'll see this kind of transformation referred to as an [activation function](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e0abf",
   "metadata": {},
   "source": [
    "<div style='display: block;margin-left: auto;margin-right: auto; width: 50%;'>\n",
    "<img src=\"logit_network_1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03689d37",
   "metadata": {},
   "source": [
    "But what happens if the effect of $x_1$ depends on the value of $x_2$? Or what if the effect of $x_1$ is curvilinear? Or what if $x_1$ and $x_2$ have an interactive effect on the outcome? Since the activation function for the logistic regression model is based on a weighted sum, so unless we explicitly add a terms like $x_1 \\times x_2$ or $x_1^2$ as features, we'll fail to capture any of these effects, which might lead us to some seriously flawed predictions. This problem becomes even more daunting when we consider models like the ones we used for text analysis, where we have thousands of predictors with all sorts of complicated relationships. This sort of problem is where neural networks can have a real advantage over simpler approaches.\n",
    "\n",
    "As with a logistic regression model, a neural network model would take the same input and output layer. Unlike a logistic regression, a neural network would also include one or more \"hidden\" layers that would infer additional weights, pass the sum of the weights through another activation function, and then pass these new values to the output layer to generate a prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a27ecc",
   "metadata": {},
   "source": [
    "<div style='display: block;margin-left: auto;margin-right: auto; width: 50%;'>\n",
    "<figure>\n",
    "    <img src=\"neural_network_1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2e4b4",
   "metadata": {},
   "source": [
    "Including these hidden layers allows a neural network to model all sorts of complexity. In fact, a sufficiently complex network can theoretically approximate **[any continuous functional relationship](https://en.wikipedia.org/wiki/Universal_approximation_theorem)** between the predictors and the outcome that you could come up with. (See [here](http://neuralnetworksanddeeplearning.com/chap4.html) for a mostly visual introduction to this concept.) This ability to model non-linear relationships is the key advantage of neural networks, and is the central reason they're so useful when modeling really complex problems like classifying images or generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25396bc",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "\n",
    "Let's take a look at a quick example of doing some prediction. The `ncbirths` dataset has information on births in North Carolina, including information about the mother and baby. Our goal here will be to predict `lowbirthweight` status using some of the variables below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004d6fc",
   "metadata": {},
   "source": [
    "| fage           | Father's age                                                                     |\n",
    "|----------------|----------------------------------------------------------------------------------|\n",
    "| mage           | Mother's age                                                                     |\n",
    "| mature         | Maturity status of mother                                                        |\n",
    "| weeks          | Weeks of gestation                                                               |\n",
    "| premie         | Whether the baby was born prematurely (>=36 weeks)                               |\n",
    "| visits         | Number of hospital visits during pregnancy                                       |\n",
    "| marital        | Mother's marital status                                                          |\n",
    "| racemom        | Mother's race                                                                    |\n",
    "| hispmom        | Hispanic origin of mother.                                                       |\n",
    "| gained         | Weight gained during pregnancy                                                   |\n",
    "| weight         | Baby's weight                                                                    |\n",
    "| lowbirthweight | Whether the baby was below 2500 grams                                            |\n",
    "| gender         | Baby's gender                                                                    |\n",
    "| habit          | Whether the mother smoked cigarettes                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbirths = pd.read_csv(\"ncbirths.csv\").dropna()\n",
    "ncbirths.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f215246",
   "metadata": {},
   "source": [
    "We'll select some features to use in our model. For this analysis, we'll use the ages of both parents, the number of doctor visits, the number of weeks of gestation, and whether the mother smoked cigarettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['mage', 'fage', 'weeks', 'visits', 'habit']\n",
    "labels = 'lowbirthweight'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ncbirths[features], \n",
    "                                                    ncbirths[labels], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564df4",
   "metadata": {},
   "source": [
    "You'll notice that we have a mixture of categorical and numeric data here. We'll need to do some pre-processing to make these categorical columns suitable for use in a machine learning model. We'll make a `ColumnTransformer` pipeline to handle the data pre-processing. This function will take a list of tuples, where the first element is just a name, the second element is a `sklearn.preprocessing` function, and the third argument is a list of column names that we want to transform.\n",
    "\n",
    "In this setup, we'll standardizing the numeric features and the categorical features will be one-hot (dummy) coded. The `remainder='passthrough'` argument means that any additional features that are not listed here will just be passed through to the next step without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "data_prep  = ColumnTransformer([\n",
    "    (\"standardizer\", StandardScaler(), [\"fage\", \"mage\", \"visits\", 'weeks']),\n",
    "    (\"onehot_encoder\", OneHotEncoder(handle_unknown='ignore'), [\"habit\"])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6852bd",
   "metadata": {},
   "source": [
    "We'll also create a function to build a neural network model. \n",
    "\n",
    "For now, you can ignore most of the code below. The most important parts are the arguments to the `Sequential` function. We'll discuss each layer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655943ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from typing import Dict, Iterable, Any\n",
    "\n",
    "def create_model(meta: Dict[str, Any]):\n",
    "    inp = meta[\"n_features_in_\"]\n",
    "    model = Sequential([\n",
    "        Input(shape = (inp, ), name='Input'),            # 1. input layer\n",
    "        Dense(16,  activation='relu', name='hidden'),    # 2. hidden layer\n",
    "        Dense(1, activation='sigmoid', name='output')    # 3. output layer\n",
    "        ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "keras_model = KerasClassifier(model=create_model,  epochs=20, batch_size=64, verbose=0,  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f488e09",
   "metadata": {},
   "source": [
    "1. The first layer is an `Input` layer. This is just going to take our features and pass them to the next layer. The only important argument here is `shape`, which should indicate the number of features we're including in the model. However, in this function, the number of features is being determined dynamically from the data.\n",
    "2. The next `Dense` layer is the hidden layer. The first argument means we'll have 16 nodes in this layer, and they'll use a [rectified linear unit activation function]() (relu). \n",
    "3. Finally last `Dense` layer is our output layer. It just takes the results and applies a sigmoid function to convert them into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9aed67",
   "metadata": {},
   "source": [
    "Finally, we'll create complete pipeline by combining our data processing steps with the keras model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893161c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "keras_pipe = Pipeline( [(\"data_prep\", data_prep), \n",
    "                        (\"neural_net\", keras_model)\n",
    "                         ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9a2cd",
   "metadata": {},
   "source": [
    "And now we'll use `fit` to fit the model to our training data, and then `predict` to predict the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = keras_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493608a6",
   "metadata": {},
   "source": [
    "We'll put this together in a confusion matrix and create a classification report to assess the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7003609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3560d",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;font-weight:bold\">Try it out</h2>\n",
    "\n",
    "\n",
    "The code chunk below just replicates what we ran before. Try making some of the following changes, then write a brief assessment of how your results compare to the results from the previous step. Are there noticeable improvements? Did the model take longer to train? Did you notice any errors or warnings?\n",
    "\n",
    "- Increase the number of  `epochs`\n",
    "- Increase the number of nodes in the hidden layer to 64\n",
    "- Add an additional `Dense` hidden layer with 16 nodes\n",
    "- Change the activation function on the hidden layer to use the `sigmoid` or  `tanh` function\n",
    "- Add additional predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39eeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['mage', 'fage', 'weeks', 'visits', 'habit']\n",
    "labels = 'lowbirthweight'\n",
    "data_prep  = ColumnTransformer([\n",
    "    (\"standardizer\", StandardScaler(), [\"fage\", \"mage\", \"visits\", 'weeks']),\n",
    "    (\"onehot_encoder\", OneHotEncoder(handle_unknown='ignore'), [\"habit\"])\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    "    )\n",
    "def create_model(meta: Dict[str, Any]):\n",
    "    inp = meta[\"n_features_in_\"]\n",
    "    model = Sequential([\n",
    "        Input(shape = (inp, )),\n",
    "        Dense(16,  activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "keras_model = KerasClassifier(model=create_model,  epochs=20, batch_size=64, verbose=0,  random_state=100)\n",
    "\n",
    "keras_pipe_modified = Pipeline([\n",
    "    (\"data_prep\", data_prep), \n",
    "    (\"neural_net\", keras_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1381d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ef96e",
   "metadata": {},
   "source": [
    "Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038396e",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "You can try using a k-fold stratified cross-validation on your model to compare accuracy scores across multiple runs of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "skf = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n",
    "scores = cross_val_score(keras_pipe_modified, ncbirths[features], ncbirths[labels], cv=skf, scoring='accuracy')\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

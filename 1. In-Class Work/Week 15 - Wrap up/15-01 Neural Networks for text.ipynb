{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fbb3d1-da93-4b6c-a574-73e09ec2b561",
   "metadata": {},
   "source": [
    "# Neural Networks II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65352a-adde-48f5-bc2a-d5691291a3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install keras_nlp\n",
    "%pip install tensorflow_datasets\n",
    "%pip install transformers\n",
    "%pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115f60f-17a7-4f73-9deb-aabce62a388d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a31b6-4671-4bc1-9b90-db7252128f13",
   "metadata": {},
   "source": [
    "## Neural Networks for Text Data\n",
    "\n",
    "Neural networks are extremely flexible, which allows you to use them for all kinds of data. We've already seen this with data that was in a 2-dimensional format with images. They can also be used for text data to do tasks such as sentiment analysis using supervised learning.\n",
    "\n",
    "\n",
    "\n",
    "Go ahead and run this (it will take a moment to finish) and we'll talk about it in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a44ba-a171-497d-a5b9-ce6ebaadc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\"\n",
    "# using the universal sentence encoder\n",
    "embed_layer_wrapper = hub.KerasLayer(url, \n",
    "                                     trainable=False, \n",
    "                                     dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458dd74-4ff6-404f-bc0a-b1eebd7a05de",
   "metadata": {},
   "source": [
    "# Reviews\n",
    "\n",
    "We'll start by reloading the IMDB movie review corpus that we used a couple of weeks ago. Just to refresh your memory: this is a subset from a larger corpus of user generated IMDB reviews. The dataset contains the full text of each review, along with a numeric label that is equal to 0 if the review was negative and 1 if the review is positive. Because this is just an example data set, there's actually an even split between positive and negative reviews here, so we have a more-or-less balanced sample of 2500 positive reviews and 2500 negative reviews to work with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fa26c-1324-45fb-a23b-a2a88131f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dddb5-9a07-40bc-91ec-6236df315551",
   "metadata": {},
   "source": [
    "Just as in previous classes, we're going to be evalauting a model here by creating separate training and testing datasets. We'll also convert these datasets to tensors in order to make it easier to work with them in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1f887-546e-4bb2-b7b8-243cd1d53dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_examples,  test_examples= train_test_split(reviews,\n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! \n",
    "\n",
    "\n",
    "# convert to tensor objects\n",
    "train_tensor = tf.convert_to_tensor(train_examples['text'])\n",
    "test_tensor = tf.convert_to_tensor(test_examples['text'])\n",
    "train_labels = tf.convert_to_tensor(train_examples['label'])\n",
    "test_labels = tf.convert_to_tensor(test_examples['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f76073-bab0-4697-9642-c8d5363e3bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Training entries: {len(train_examples)}, test entries: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cac3d0-f5ec-49ff-8cbe-8d8c1995e064",
   "metadata": {},
   "source": [
    "## Embeddings and Transformers\n",
    "\n",
    "In a previous class, we trained a naive bayes classifier to distinguish positive from negative IMDB reviews with a fairly high degree (~84%) accuracy. \n",
    "\n",
    "Now, we're going to try to do the same task using a neural network trained on a sentence embedding model. **Text Embeddings** represent one way that analysts can move away from the bag-of-words model to create classifiers that can account for things like word order, synonyms and antonyms and complex grammatical relationships.\n",
    "\n",
    "Word embedding models can take strings of text and convert them into a \"dense\" vector of numbers whose values reflect some kind of abstract meaning. The precise method for creating them will be different depending on the model, but a typical approach is to use some text as training data and then \"predict\" some randomly held out terms or context. Words that have similar meanings will end up with similar weights in the model, because they'll occur in similar contexts.\n",
    "\n",
    "In a well-trained word-embedding model, words with similar meanings will have similar values (<a href='https://projector.tensorflow.org/'>there's a good visual representation here</a>). Instead of using a bag-of-words as our input for a classifier, we can pass our text through an embedding model to get a representation that can account for things like synonyms and context.\n",
    "\n",
    "A newer approach, called a transformer, goes a step further and trains a model that can encode both semantic meaning and information about the context in which a word occurs, which means we no longer need to rely on the bag-of-words assumption when working with text. Transformers can take a full sentence and spit out a series of numbers that can function as a sort of numeric representation of meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64c139-d717-4abb-9f11-87c405a42d8c",
   "metadata": {},
   "source": [
    "The `embed` object we downloaded earlier is a pre-trained transformer model that is built for general-purpose applications. It takes a list of strings as inputs and returns a vector of 512 numbers that represent that sentence's \"location\" in a 512 dimension space. Here's an example of embedding a sentence and then getting the first 10 elements of the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b3988-e3d0-41b7-8092-049b5f55affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding a sentence about catci and looking at the first 10 elements\n",
    "\n",
    "embed([\"The rattail cactus is native to Mexico.\"])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd881165-a97e-45cd-acee-a8e85ee89bc6",
   "metadata": {},
   "source": [
    "Note that we don't really need to do much (or any) pre-processing of the texts in order for this to give good results. The model is trained on mostly unprocessed text, so things like stripping punctuation or removing stopwords often does more harm than good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dae75a-04fc-448b-a3b7-6d54a87d301c",
   "metadata": {},
   "source": [
    "To illustrate what why the embedding is useful, we can use a little code from the <a href='https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder'>online documentation</a> that will allow us to visualize the similarities between the embeddings produced by different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453094c-f41f-4ab3-8cca-0ab8f6a3cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embed(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4339248-63d3-471d-9efa-00f74ac80be8",
   "metadata": {},
   "source": [
    "Below are some sentences from different wikipedia entries. The first two are from the entry on *Citizen Kane*, the last two are from entries on cacti. Note that the terms in both groups share very few terms overall, but take a look at their similarities as measured by the inner products of their respective embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42284b-8f99-47d2-9a06-ce25eddccf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_and_plot([\n",
    "    # two sentences from the Wikipedia entry for citizen kane\n",
    "    \"Citizen Kane is often cited as the greatest film ever.\",\n",
    "    \"Hollywood had shown interest in Welles in 1936.\",\n",
    "    # sentences from entries on cacti\n",
    "    \"The rattail cactus is native to Mexico.\",\n",
    "    \"Prickly pears are frequently found around California.\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b453b-8aad-40b3-9810-0bf114481fc7",
   "metadata": {},
   "source": [
    "In essence, text embeddings give us a more flexibile way to represent text that can account for nuanced aspects of meaning and context, so that sentences about the same general idea are \"close\" in the embedding space even if they share none of the exact same terms. Feeding these inputs - instead of a simple bag of words - into a machine learning model, can allow us to make more effective use of the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e000-fe1b-43a3-a9b0-fc5b4cccd163",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Now, let's fit a model to predict movie reviews that uses the embedding model. We'll use the embedding layer as our input layer and then include two hidden layers and a sigmoid output layer that will return our predicted probability of a review being negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc22b-1b82-4650-a744-26e3842d2604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "   \n",
    "    embed_layer_wrapper,\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf1c5c-3195-46d2-a084-18c6162ebe29",
   "metadata": {},
   "source": [
    "After that, we compile our model and then train it for 15 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dbed4-ad1a-48a6-816a-d78b3ecaedbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_tensor, \n",
    "                    train_labels,\n",
    "                    epochs=15,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(test_tensor, \n",
    "                                     test_labels),\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3d89e-993d-45d4-a5cc-81ec4116fce9",
   "metadata": {},
   "source": [
    "Now we can generate some predictions and look at our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c7891-ae48-49d5-abcd-21c19ff63612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions from test data\n",
    "preds = model.predict(test_tensor).flatten()>=.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed930a3-764b-4538-9921-0e5ac509f2de",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"color:red;font-weight:bold\">Question 1 </h2>\n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">Create a confusion matrix and classification report from the predictions and assess the quality of the classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8669f70-3bdd-4db6-82a9-63b104b4b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844f15b-406c-46a4-8cd0-a446dea60eb1",
   "metadata": {},
   "source": [
    "How does this do? Does it outperform the naive bayes classifier? Why might this be? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f80381-6aaa-4942-b0e9-423ad700cdc0",
   "metadata": {},
   "source": [
    "## Changes to the Model\n",
    "\n",
    "We can make changes to the model to add more layers, use more nodes, train it for longer, or even use a different kind of model. This is part of the overall process for finding the model that has the best performance in terms of accuracy. In reality, we would do these steps many, many times, tuning our model so that it is as good as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff26f9-8455-4790-8c34-c1c36d909d4e",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"color:red;font-weight:bold\">Question 2 </h2>\n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">Change something about the model above and compare your results.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac44f48-46f2-4f09-b45e-23fb2bfd0980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64d96b7-3c44-40c8-8055-578f21947969",
   "metadata": {},
   "source": [
    "(Some options are: add an additional hidden layer, run the same model for more epochs, add more nodes to one or more of the layers, or add <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout'>dropout</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f940-d869-48a1-ae1b-4def25ab9dc0",
   "metadata": {},
   "source": [
    "### Pre-built models from Hugging Face\n",
    "\n",
    "In reality, the full IMDB reviews corpus is much larger than what we've been using here, so we would also want to use that data in its entirety for a real world application, but since that takes a while to train, we can use a pre-made model that was trained on this data set to get a sense of how well we could do if we did some more fine-tuning.\n",
    "\n",
    "The [Hugging Face Hub](https://huggingface.co/models) has many models that have been pre-trained for you to use. One of the models hosted there is a <a href='https://huggingface.co/aychang/roberta-base-imdb'>sentiment classifer that was trained on the entire IMDB corpus</a>. We can load this model and see how it performs on our held-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bdf53-ad36-41f8-9dbf-d0245d434650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiny_bert = pipeline(\"text-classification\", \"arnabdhar/tinybert-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bde1a-df2d-469d-b063-ba36b689708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to list since thats the input format this model uses\n",
    "test_list = test_examples['text'].tolist()\n",
    "\n",
    "# applying the model\n",
    "\n",
    "results = tiny_bert(inputs =test_list, max_length=512, truncation=True)\n",
    "# looking at the first five results\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880fb6f-bfdf-4113-92cf-6be63fe4e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformatting to match our original test labels \n",
    "tiny_bert_preds = [int(i['label']==\"POSITIVE\") for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4422e8-fb46-493f-aba8-b28a0a8d2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, tiny_bert_preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                              target_names=['Negative', 'Positive']))\n",
    "\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(test_labels, tiny_bert_preds))#\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(test_labels, tiny_bert_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684505cd-6bc2-4a6a-8cca-bca2fd36c5b9",
   "metadata": {},
   "source": [
    "## Other Types of Sentiment\n",
    "\n",
    "The nice thing about these models is that they are also pre-trained to do different types of sentiment analysis. For example, let's take the Distilbert-base-uncased-emotion model. This provides scores for emotions such as joy or anger. Here's an example of getting the emotions expressed in the first 100 rows the the reviews data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b7962-541b-45a0-b596-3603dc1f760f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\",\n",
    "                      model='bhadresh-savani/distilbert-base-uncased-emotion', \n",
    "                      top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35234530-74a5-46a2-9a51-57926bedd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"This is an angry sentence\", \n",
    "       \"I'm very afraid\",\n",
    "       \"I'm depressed!\"\n",
    "]\n",
    "\n",
    "prediction = classifier(lst, truncation=True, max_length=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d9f39-8a99-49d7-961b-a93c0effe63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41d36e-c8a1-41d7-8322-5866ff0c1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_prediction = pd.concat([pd.DataFrame(i) for i in prediction])\n",
    "emotion_prediction.groupby('label').agg({'score':['min','max','median','mean']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d44fd4-7bc8-4a83-9486-747d961b8f78",
   "metadata": {},
   "source": [
    "You can check out some other options on the hugging face <a href='https://huggingface.co/models'>models page.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae6187-0056-4364-8c0c-3cd0de1472d7",
   "metadata": {},
   "source": [
    "# BertTopic\n",
    "\n",
    "[BertTopic](https://maartengr.github.io/BERTopic/api/bertopic.html) is a transformer-based method for topic modeling that works especially well on short texts. You can read more about the algorithm [here](https://maartengr.github.io/BERTopic/algorithm/algorithm.html)\n",
    "\n",
    "You can try it out with the code below, but be warned that it will take a while to run. For the sake of a speedier model, we'll work with just the headlines and first few sentences from the Fox/CNN news articles from a couple weeks ago. The BertTopic package also provides several built-in functions that make it easy to visualize and explore results, some of which are demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853766bf-a905-4768-b218-084619693032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install packages:\n",
    "#%pip install BERTopic\n",
    "#%pip install SentenceTransformer\n",
    "#%pip install UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753860c-c775-4b93-b077-541599415432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERTopic (this is very slow sometimes)\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# Loading the 4,000 articles sampled from CNN and Fox\n",
    "articles = pd.read_csv('https://github.com/Neilblund/APAN/raw/main/news_sample.csv')\n",
    "# doing some truncation and combining here just to make the model run faster\n",
    "docs= [' '.join(sent_tokenize(i + \" \"+ j)[:5]).strip() for i, j in  zip(articles.headline, articles.text)]\n",
    "# embedding the texts (typically the slowest part)\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "# setting the UMAP model (this makes results reproducible)\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=119)\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic(umap_model = umap_model).fit(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5bbc5-2458-444e-8134-b619cbaa0a39",
   "metadata": {},
   "source": [
    "From here, we can try out some of the built-in functions for exploring the results of the topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc8f1c-1605-4d49-aa42-569243f28f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all topics and the top terms associated with each one\n",
    "topic_model.topic_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514aa6d-2f21-421b-bcd9-75c4c2cebc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view representative documents for topic 0\n",
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a2f33-03f6-445d-9099-430b989f661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09567cb-c39c-4833-98b9-51be28311341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap showing textual similarity of each topic \n",
    "topic_model.visualize_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b810d9a-f849-48f0-b320-c494cd9f9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distance between topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667e174-282b-4039-9877-9ed9fb93fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected topics associated with each outlet (normalized by their overall frequency)\n",
    "topics_per_class = topic_model.topics_per_class(first_200, articles.source)\n",
    "topic_model.visualize_topics_per_class(topics_per_class,  topics=[1, 3, 8, 11,12, 13], normalize_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8a9d4-b4f1-431f-ab48-654741396ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents in space \n",
    "topic_model.visualize_documents(articles.headline, embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e68ed-8e91-4abe-9c02-8c17e93a7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms for selected topics\n",
    "topic_model.visualize_barchart([0, 1, 2, 3,11, 10], n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc40ab3-30ca-42e1-a066-cdcef9a7d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view topics over time (this model also allows topics themselves to vary slightly depending on the time period)\n",
    "# see https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html\n",
    "topics_over_time = topic_model.topics_over_time(docs, articles.date, nr_bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e356d-a3ef-4a8a-8dfc-0e703128b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 topics\n",
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=5)\n",
    "# or try this to see specific topics:\n",
    "#topic_model.visualize_topics_over_time(topics_over_time, topics = [0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b2fbe-8a9c-4ad9-8661-698130e0d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topic distributions across individual terms\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs, calculate_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfcbd1-7aa9-4fb5-9387-cba808c2e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the topic-word distribution for the first document:\n",
    "df = topic_model.visualize_approximate_distribution(docs[0], topic_token_distr[0])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

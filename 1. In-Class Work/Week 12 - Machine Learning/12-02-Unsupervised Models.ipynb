{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f32d3e-dcd8-4f37-99d7-c3ab475aab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3239b4-27b5-44dd-8956-4df8256835ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05822747-f163-4b1e-b910-9d5d58c81a3b",
   "metadata": {},
   "source": [
    "# Unsupervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089c192-c3d8-4d65-8d73-44d25db298b3",
   "metadata": {},
   "source": [
    "\n",
    "In supervised learning, we used known labels to predict unknown labels. In unsupervised learning, we don't have any labeled data to work with, and our primary goal is to identify structures or simplify our data so we can more easily summarize it, visualize it, or use it in a supervised model. In contrast to supervised learning, these methods are often more appropriate for exploratory analyses where you don't necessarily have a goal other than finding out how things \"work\" in your data. As a result, there's a lot more room for interpretation than you might encounter in other data analysis tasks.\n",
    "\n",
    "We'll talk about two widely used unsupervised methods that are often used together: **k-means clustering** and  **principal components analysis**. (note: we actually talked about these briefly last week, but we'll explore them in a little more depth here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d665fc-c9ea-454a-b8c3-0dd9b39df1fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clustering data with K-means\n",
    "\n",
    "<p>Clustering methods like k-means are a way of identifying a set of groups with similar characteristics in some data. K-means clustering is one of the simplest and most commonly used methods for clustering. The <b>K</b> in \"K-means\" is some number of groups chosen by the researcher. </p>\n",
    "<p>The data set below includes some data on penguins recorded by biologists working in the Palmer archipelago in Antartica. The <code>bill_depth_mm</code> and <code>bill_length_mm</code> are measurements of the penguin's bills. Looking at the scatter plot of these measurements, you can probably eyeball some clear \"groups\" in the data. Reasonable people might disagree on whether there were 2 groups or 3, or maybe even 4, but there are clusters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e0b61-3333-4b09-8241-6861ab8f2568",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv').dropna()\n",
    "plt.scatter(x=penguins['bill_length_mm'], y=penguins['bill_depth_mm'])\n",
    "plt.xlabel('bill length')\n",
    "plt.ylabel('bill depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd54d3f-9441-4dcd-a11f-cd6635aa9078",
   "metadata": {},
   "source": [
    "Instead of just trying to eyeball it, we can use K-means clustering to identify a set of optimal clusters for this set of features. Specifically, K-means attempts to identify the cluster assignments that minimizes the total within-cluster sum of squares. Visually, you can think of it as trying to find clusters where all of the points are as close together as possible.\n",
    "\n",
    "To use K-Means, we first need to mean-center and standardize our features so they all have mean = 0 and a standard deviation of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c72e59-d4ce-4379-a2a2-569b4d147e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = ['bill_length_mm', 'bill_depth_mm']\n",
    "X = penguins[features] \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898628a-73bf-4e81-9a47-85cb5a568c30",
   "metadata": {},
   "source": [
    "Note that scaling doesn't really change anything about the relative positions of any points in our dataset, it just ensures they are both centered on zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce0bef-95cf-4575-ab88-5f5f19aca085",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_scaled[:,0], X_scaled[:,1])\n",
    "plt.xlabel('scaled bill length')\n",
    "plt.ylabel('scaled bill depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b5f4c-2835-4c4a-b91c-abba3d3d5040",
   "metadata": {},
   "source": [
    "Now we'll apply K-means clustering on the scaled data. We'll initialize a model by calling `KMeans` along with any additional options we want to specifiy. In this example, I'm going to assume there are three clusters, and I'll use the `random_state` variable to ensure reliability. Finally, I'll set the model up to use 10 random initalizations (this is a way to avoid becoming [stuck in a local minimum](https://stats.stackexchange.com/questions/487160/what-does-it-mean-for-the-k-means-algorithm-to-be-trapped-in-a-local-minimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d1b00-9472-4885-a602-2c1a3c9eda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=99, n_init=10)\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5c5ed-7115-4e36-88b9-daa35a55d05e",
   "metadata": {},
   "source": [
    "The `intertia` attribute is the total within cluster sum of squares value that the algorithm is attempting to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e5801-b6bc-4203-9bc9-8b997253c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370bbe2-1475-4cf5-9968-9640dbfa36c5",
   "metadata": {},
   "source": [
    "The `cluster_centers_` attribute is a K x J array with one row for each cluster and one column for each feature. The values are just the average of the (scaled) features within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53106616-314d-4956-83cc-2e992103e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee58210-cfc6-4dc9-8673-3233dfc7a77a",
   "metadata": {},
   "source": [
    "Putting the results in a data frame and adding the feature names can make this a little more readable, and can give you a sense of where each cluster is located in the data. ex: cluster 0 is on the low end of bill length and has moderate depth, while cluster 1 has moderate length and low bill depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5836e3-c1db-4e39-a5f7-8c7f9a3969a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(kmeans.cluster_centers_, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975d85b-e545-4b2c-9be4-de2899c18a9b",
   "metadata": {},
   "source": [
    "The `.labels_` attribute of our fitted model has the cluster assignments. This should have a numpy array with one element for each observation in our data, and the numbers should correspond to the value of `K`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c47b1-7534-460b-a0f9-343399710f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23fbe6-0f96-46bf-a403-7231ec9ac920",
   "metadata": {},
   "source": [
    "Right now, these are stored as a numpy array, but we can add them back to our original data frame as a new column to have them in Pandas format, then we can use this new column in visualizations and other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d57cd-50c7-415a-a39b-6f132663db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e29e2-c1fb-40bc-9681-e11028d9af53",
   "metadata": {},
   "source": [
    "Using a color map to assign colors to each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5024822-ba20-4251-90d7-0e530d3a6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=penguins, x='bill_length_mm', y='bill_depth_mm', hue='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a69ec2-b41d-48e4-8539-f77c7065fac7",
   "metadata": {},
   "source": [
    "Note that, in this instance, the clusters we identified roughly correspond to the different species in the penguins sampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db1200-de5c-4513-aa3f-35dedc5440d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=penguins, x='bill_length_mm', y='bill_depth_mm', hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f513f01-d28b-4fe4-85b2-650b7511e4bb",
   "metadata": {},
   "source": [
    "For the purposes of this example, we chose to use just two features and 3 categories, but in most practical scenarios where you would want to using K-means clustering, you'll have a lot more variables you're interested in. Foruntately, the logic here generalizes readily to higher dimensional data. If we want to visualize the results of a cluster analysis that uses more than two variables, we might consider using a dimensionality reduction method like the one we're discussing in the next section so we can get a two-dimensional representation of a higher dimensional data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f21ad9-6736-46d6-ac1e-6be8936a328a",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;font-weight: bold;\">Question 1: Use <code>K-means</code> clustering on the <code>body_mass_g</code> and <code>flipper_length_mm</code> variables. Use your best judgement to select a value for <code>K</code> and create a scatter plot to visualize your results. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961257d8-5484-4511-a2e2-2299ba4b9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42f22a-740e-4c16-9c64-a11caac85d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choosing values for K: the elbow method\n",
    "\n",
    "Picking a number of clusters is often more of an art than a science. There are some wrong choices (it wouldn't make sense to use K=1) but there are usually multiple good values. The elbow method is a hueristic for picking a \"good\" value for `k` in K-means clustering. It doesn't give us a definitive answer, but it can help guide decision when you're not sure whether there are 4 groups or 5. To use it, we run K-means multiple times with increasing values of K and then we plot the total within cluster sum of squares for each iteration. The \"elbow\" is the point where the line bends sharply. Since the within cluster sum of squares is our measure of cluster quality, this \"elbow\" represents the point where we start to see diminishing returns for increasing values of K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62353de7-e47b-4215-923f-0876590f026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty list\n",
    "wcss = []\n",
    "\n",
    "# test out values from 2 to 20\n",
    "for k in range(2, 20):\n",
    "    # get K clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=99, n_init=5)\n",
    "    kmeans.fit(X_scaled)\n",
    "    # get the WCSS value (stored in the \"interia_\" attribute:\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435fa2e-82b4-4d37-abee-23095ae7daf5",
   "metadata": {},
   "source": [
    "Based on the results, it looks like 3 or 4 clusters is plenty here. Additional values of \"K\" beyond that don't seem to improve things much. This makes sense given that we're only using two features here - there's just not a lot of complexity, and the clusters we identified already do a good job of explaining the variations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258e0ed-f103-4e1b-a363-1aa71a869c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "plt.plot(range(2, 20), wcss, marker='o')\n",
    "ax.locator_params(axis='x', nbins=20)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12eaeab-bff7-4713-b2cd-4142d9c8b234",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Note</h3>\n",
    "K-means clustering is one of the most widely used methods for cluster analysis, but there are a variety of alternative models that may perform better for very large data sets, or in cases where clusters are defined by complex non-linear relationships. You can read a little more about some alternative clustering methods supported by scikit-learn <a href='https://scikit-learn.org/stable/modules/clustering.html'>here</a>. Some of these models, such as <a href='https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html'>DBSCAN</a> and <a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.cluster.ward_tree.html'>Ward clustering</a>, will automatically choose a number of clusters based on some characteristics of the data. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c9158-79a4-4a4c-830b-2200291983b0",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Principal Components Analysis is an unsupervised dimensionality reduction technique that can help simplify large datasets with lots of correlated variables. If we have an `N x K` matrix of correlated variables, PCA will return a new `N x K` matrix with these properties:\n",
    "\n",
    "1. The columns will be uncorrelated with each other.\n",
    "2. The columns will be ordered by their importance. In other words, the first column will be some linear combination of the variables that explains the most variation in the data, the second column will explain the second most variation, and so on.\n",
    "\n",
    "Taken together, these two properties mean that we can often use PCA to take a large number of correlated variables and create 2 or 3 new variables that capture most of the variance in our data. We can then use these results as inputs for a supervised machine learning model, or to do something like visualize a complex data set in a simple scatter plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5165406-c977-4d63-ae1f-825a28158592",
   "metadata": {},
   "source": [
    "For this section, we'll use some data on European political parties from the 2024 round of the [Chapel Hill Expert Survey](https://www.chesdata.eu/ches-europe) (CHES). The CHES project attempts estimate party positions and ideology by aggregating expert opinions. in essence they might ask 10 experts on UK politics to rank each of the major parties in terms of their support or opposition to increasing immigration, and then they would use the average responses to this question as a measure of Labor's immigration views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d2ae3-615d-4829-b094-e3b4d30ab8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ches = pd.read_csv(\"ches_data.csv\")\n",
    "ches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f48ac-5a66-4088-9ff3-e4a925e12379",
   "metadata": {},
   "source": [
    "Here are descriptions of the variables used below:\n",
    "\n",
    "| Variable name  | Variable                                                                                                       | Values                                                                                                                                                                  |\n",
    "|----------------|----------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| spendvtax      | Position on IMPROVING PUBLIC SERVICES VS. REDUCING TAXES during 2024                                           | 0 = strongly favors improving public services  <br>10 = strongly favors reducing taxes                                                                                  |\n",
    "| redistribution | Next, where did these political parties stand on REDISTRIBUTION in 2024?                                       | 0 = strongly favors redistribution<br>10 = strongly opposes redistribution                                                                                              |\n",
    "| climate_change | What was their position towards CLIMATE CHANGE POLICIES during 2024?                                           | 0 = strongly supports climate change policies even at the cost of economic growth<br>10 = strongly supports economic growth even at the cost of climate change policies |\n",
    "| lgbtq_rights   | Position on policies supporting LGBTQ+ RIGHTS (e.g. marriage equality, adoption rights, transgender rights)    | 0 = strongly supports LGBTQ+ rights<br>10 = strongly opposes LGBTQ+ rights                                                                                              |\n",
    "| eu_position    | How would you describe the GENERAL POSITION ON EUROPEAN INTEGRATION that the party leadership took during 2024 | 1 = strongly opposed<br>7 = strongly in favor                                                                                                                           |\n",
    "| protectionism  | Position towards TRADE LIBERALIZATION/ PROTECTIONISM                                                           | 0 = strongly favors trade   liberalization<br>10 = strongly favors protection of domestic producer                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221d706-ae47-4923-afc1-9ce925d878e9",
   "metadata": {},
   "source": [
    "We'll start by selecting 4 features of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a137cd-b39a-4559-a12d-90f314ac602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = ['spendvtax', 'redistribution','climate_change', 'lgbtq_rights']\n",
    "correlation_matrix = ches[features].corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618df2e-5397-4a34-a127-cc8a59ee8b08",
   "metadata": {},
   "source": [
    "We can start by examining a correlation matrix for these features. You'll notice that some of them are strongly correlated with each other. In some cases, correlations are strong enough that we might think that including them together in a model is potentially redundant. If we know a party's position of taxes and spending, we *probably* can make a good guess as to their position on taxes and spending because the two values are so strongly related. One way to think about the usefulness of dimensionality reduction is that it gives us a way to identify and eliminate this kind of redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b93e0-a74e-421d-a723-d2c9f0fbd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using seaborn to visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00b488-7272-44ab-8cb3-eeed8f22cbcc",
   "metadata": {},
   "source": [
    "To use PCA, we'll start by scaling the features (notice a trend?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0f85b-aa56-4fdb-8a03-bd6749aa4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = ches[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc606a26-4a5f-47cf-9308-059bbdd998a1",
   "metadata": {},
   "source": [
    "We'll use initialize the PCA model, then use `pca.fit` to fit it to the data. Finally we'll use `transform` to apply the fitted model to the variables in X_scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3adb84-7072-40ca-86f5-078ff0e33711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "prcomp = pca.fit(X_scaled)\n",
    "compvalues = prcomp.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea41380-d063-412e-99e1-4c98955494c3",
   "metadata": {},
   "source": [
    "We'll start by taking a look at the characteristics of the fitted model, then we'll try using `compvalues` for an analysis. \n",
    "\n",
    "The `explained_variance_ratio_` attribute of our fitted PCA model is an array that give the proportion of variance explained by each component of the new PCA matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288c6f6-c5b1-4d3e-a202-16f7efec8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcomp.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc3e76-e484-4eff-aab6-d0103317fdc2",
   "metadata": {},
   "source": [
    "You can read this as saying: \"the first component explains about 72% of the variation, the second explains about 23% the third explains 3%... and so on. So, for this set of variables, we can explain over 90% of the variation in our data using just the first two components of this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d5883-98d3-4925-b4c2-5513a589e615",
   "metadata": {},
   "source": [
    "We can also use the `prcomp.components_` to see the \"loadings\" or the importance of each feature in each component. Larger absolute values indicate that a feature is more strongly associated with a particular component, and negative/positive signs indicate that a feature is either positively or negatively associated with that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2739bbc-2ea1-43a7-92e6-457b9ca72903",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcomp.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362da663-2852-43a3-a579-3c3ac6f8d5da",
   "metadata": {},
   "source": [
    "We get this as a numpy array, but we can make it a bit more readable by converting it to a Pandas Dataframe and naming each of the columns. Each row here represents a single component, and each column represents a specific feature. The first dimension is relatively equally associated with all for values, while the second dimnesion is positively associated with `spendvtax` and `redistribution` but negatively associated with `lgbtq_rights` and `climate_change` views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83473b-8cfd-4dc7-8190-2e3a2c539a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(prcomp.components_, columns = features)\n",
    "components_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56816b6-9472-4e60-a065-1f9a2efa0dd6",
   "metadata": {},
   "source": [
    "Since the first two principal components explain most of the variation in our data, we should be able to get a reasonable summary of our data by just visualizing observations on those first two component values. We'll extract those components and put them in a data frame, then we'll join this back up with our original data set so we can use the components for more analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae241642-170d-4b56-a349-6eaae92a213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_frame = pd.DataFrame(compvalues[:,:2], columns =['component 1', 'component 2'])\n",
    "df = pd.concat([ches, components_frame], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbb6ef-983e-4992-bfd3-abeeccc8564c",
   "metadata": {},
   "source": [
    "To visualize the results, we'll use plotly to create a visualization that shows each party's placement on these two axes. We'll also use a `hovertemplate` to display additional information about each point when hovering. This should allow us to get a good idea of what each component is picking up on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39221f-4287-4155-91e9-cfdc921880d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(df, \n",
    "           x='component 1', \n",
    "           y='component 2',     \n",
    "           custom_data =['party','family', 'country', 'spendvtax', 'redistribution', 'lgbtq_rights',\n",
    "                        'climate_change'],\n",
    "           width=600,   \n",
    "           height=500 )\n",
    "fig.update_traces(hovertemplate=\n",
    "                  \"<b>%{customdata[0]}</b>\"\n",
    "                  '<br>Family: %{customdata[1]}' +\n",
    "                  \"<br>Country: %{customdata[2]}\" +\n",
    "                  \"<br>spendvtax: %{customdata[3]:.1f}\" +\n",
    "                  \"<br>redistribution: %{customdata[4]:.1f}\" +\n",
    "                  \"<br>lgbtq rights: %{customdata[5]:.1f}\" +\n",
    "                  \"<br>climate change: %{customdata[6]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362d786-8f3c-4473-be94-d12c46394d35",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3 style=\"color: red;font-weight: bold;\">Question 2: Use K-means clustering to identify groups of parties using the features from the previous steps. Create a color-coded version of the scatter plot above, where each point is color coded according to its cluster membership.</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cada9ed-96b7-4a2d-a9e2-584733986464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc93a775-34b2-47f5-8693-95ceb9861fe3",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color: red;font-weight: bold;\">Question 3: Repeat the above steps, but include <code>protectionism</code> and <code>eu_position</code> as additional features before performing PCA. Discuss the changes in your results and what they might mean.</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3da2-ced5-4da8-952f-129860b56c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b134b8cc-ca29-4df6-96cc-4b5da7d1174f",
   "metadata": {},
   "source": [
    "### Basic steps for using PCA: \n",
    "1. Select a set of numeric features that are correlated (or, if they're categorical, convert them to numeric by dummy-coding)\n",
    "2. Center and scale the features. \n",
    "3. Use your best judgement to select a value for K (you can always change it if your results don't seem right)\n",
    "4. Perform PCA and examine the results.\n",
    "5. Choose one or more principal components to use for your analysis. If you're trying to make a visualization, you probably just need the first two, but you could use more if you're using PCA to process features for a supervised model. \n",
    "\n",
    "Keep in mind that using less that the total number of available features effectively means you're throwing out some data. But, if a couple of features can already explain most of your variation, you're not losing much information and you gain the benefits of being able to simplify your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

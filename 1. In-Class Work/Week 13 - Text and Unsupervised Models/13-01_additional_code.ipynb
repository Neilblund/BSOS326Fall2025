{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de206fb2-3faa-463b-849a-4cf6cd181f5d",
   "metadata": {},
   "source": [
    "# Extra code from Tuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b381c-428f-4adf-a25e-6040a7c6c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718f0cc-073a-4471-a906-20748c6e31cc",
   "metadata": {},
   "source": [
    "# Reloading a trained pipeline\n",
    "\n",
    "The model below is more-or-less the one we used in Tuesday's class. It's been saved as a \"pickle\", a method of storing python objects that are too complex to represent as a .csv file. You can see an example of how this was saved in the `serializeLDA.py` file in this folder.\n",
    "\n",
    "\n",
    "For now, though, we'll just read in the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15946b6-25e3-427b-93c8-000f0d1050af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newstopics15.pickle', 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    lda_pipeline = pickle.load(file)\n",
    "    \n",
    "lda= lda_pipeline['lda']                                      # get the LDA model\n",
    "features = lda_pipeline['vectorizer'].get_feature_names_out() # get the words associated with each index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ffde2-94f5-47a1-bbef-6c4106473e86",
   "metadata": {},
   "source": [
    "We can use this pipeline to get the topic distribution for new texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57f9f2-cbfd-4198-815e-d9e6f51065b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdoctopics = lda_pipeline.transform(['This is a text about the covid 19 virus.', 'This is about the border and immigration'])\n",
    "newdoctopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b4a10-aa12-414e-9df7-3b26057e61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the most common topic in each of the two documents\n",
    "[np.argmax(i) for i in newdoctopics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6f346-2804-446d-9417-ad531d3c742a",
   "metadata": {},
   "source": [
    "We can also retrieve the top terms associated with each topic using the `getTopicTerms` function in the text_functions script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b635b7-9e75-4a70-9d22-4517b2751614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_functions import getTopicTerms\n",
    "top_terms = getTopicTerms(lda, features, n_terms=10)\n",
    "top_terms.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9cf87-39b4-43ba-a253-a78f9406cff4",
   "metadata": {},
   "source": [
    "## Refitting a model\n",
    "\n",
    "This is somewhat redundant, since we already trained the model on these documents, but the fitted document-topic distributions aren't saved in the `lda_pipeline` object, so we'll need to import the articles and then use `transform` to apply the topic model to our documents again.\n",
    "\n",
    "First, we read in the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bd79d-c711-4184-a67e-90408874095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('https://github.com/Neilblund/APAN/raw/main/news_sample.csv')\n",
    "articles['headline'] = articles.headline.str.strip()\n",
    "articles['hyperlink']=articles.apply(axis=1, func = lambda x: f'<a href={x.url}>{x.headline}</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0dd2b-a691-41e9-a827-bed6c2d88b4a",
   "metadata": {},
   "source": [
    "Next, we'll fit the LDA model to our articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51a7f6-ca4a-4617-ad60-fa564f0d8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doctopics = lda_pipeline.transform(articles['text'])\n",
    "topic_memberships = pd.DataFrame(doctopics)\n",
    "topic_memberships.columns = [\"topic \" + str(i)  for i in topic_memberships.columns ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd7077-9698-42be-99b1-fc0e76156d8a",
   "metadata": {},
   "source": [
    "# Getting topics associated with a particular source\n",
    "\n",
    "How would we identify the topics most strongly associated with Fox News or CNN? One way to do this would be to just group by source and then calculate the average % for each topic in Fox vs. CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836056ea-9a48-4f5d-9094-e0f2446a78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_source = pd.DataFrame(doctopics).groupby(articles['source']).mean(numeric_only=True).transpose().reset_index(names='topic')\n",
    "topics_by_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd8c04-9b21-494c-a2a4-716b3008fc66",
   "metadata": {},
   "source": [
    "Better yet, we could calculate the logged ratio of \"% topic k in Fox News articles compared to % topic k in CNN articles\". This will make it so that topics more associated with Fox News will have a positive log-ratio, whereas topics associated with CNN will have a negative ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f70de-9b85-4b25-b741-2c6e7fabd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_source['logratio'] = np.log(topics_by_source[\"Fox News\"]/topics_by_source[\"CNN\"])\n",
    "topics_by_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f414df-71b8-4358-88c3-e2a48229696c",
   "metadata": {},
   "source": [
    "Now we can visualize the results as a bar graph. We'll also add some annotations to each bar to show the keywords for each topic and color-code the results to make things a little more visually interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844c72b-2441-40cb-a793-f167538b43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from the top terms associated with each topic: \n",
    "labels = top_terms.groupby('topic').head(n=5)[['keywords','topic']].groupby('topic').aggregate(lambda x: ', '.join(x))\n",
    "topics_labeled = pd.merge(topics_by_source, labels, on = 'topic').sort_values('logratio').reset_index()\n",
    "\n",
    "topics_labeled['topic'] = topics_labeled['topic'].astype('str')\n",
    "\n",
    "# then create a plot using the log ratios for each topic \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "g = sns.barplot(data=topics_labeled,\n",
    "                 y= 'topic',    \n",
    "                 hue='logratio',\n",
    "                x='logratio',dodge=False, palette='viridis')\n",
    "ax.set(xlabel='Topic associations CNN (negative values) \\nvs.\\n Fox News (positive values)', ylabel='topic')\n",
    "g.legend_.remove()\n",
    "\n",
    "for index, row in topics_labeled.iterrows():\n",
    "    if row['logratio']<0:  \n",
    "        ax.text(.01, index,row['keywords'], fontsize=9) #add tex\n",
    "    else:\n",
    "        ax.text(-.01, index,row['keywords'], fontsize=9, horizontalalignment = 'right') #add tex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10de272-6ad2-48c8-ad36-d40edd4e15c2",
   "metadata": {},
   "source": [
    "# Making a custom table\n",
    "\n",
    "Next, we might like to have a look at some example articles associated with each of our topics. We'll also use a little HTML formatting to make a hyperlink that you can click to read the article itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb49e92-b14f-4a34-8159-ecf5f3186706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the document-topic distribution to our original data frame of articles:\n",
    "articles_with_topics = pd.concat([articles, topic_memberships], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2c6d9-3730-40ca-92c0-4ae877420948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_terms = 10\n",
    "n_docs = 2\n",
    "top_documents = []\n",
    "top_index = topic_memberships.columns.values.tolist()[:15]\n",
    "for i, label in enumerate(top_index):\n",
    "    top_n_documents =  articles_with_topics.sort_values(label, ascending=False).head()\n",
    "    terms={ 'topic' : i,\n",
    "           'mean proportion' : np.mean(topic_memberships[label]),\n",
    "        'docs' : '<br>'.join([i for i in top_n_documents['hyperlink'].to_list()[:n_docs]]),\n",
    "        'terms' : ', '.join([features[j] for j in np.argsort(lda.components_[i])[::-1][:n_terms]]) \n",
    "    }\n",
    "    top_documents.append(terms)\n",
    "pd.DataFrame(top_documents).reset_index(drop=True).style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f1591-1b6b-4bd3-864e-70e2f688d7e2",
   "metadata": {},
   "source": [
    "# Getting topics over time\n",
    "\n",
    "I might also want to see how coverage changes over time. For instance, topic 7 is mostly related to abortion and Roe v. Wade. Maybe I want to see how coverage of that issue has increased or decreased over time, or identify where it peaks. Here's one way I could do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6cf70-5f3e-4c17-b9dc-cd531060e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the date to a date_time object type\n",
    "articles_with_topics['pubdate'] = pd.to_datetime(articles_with_topics['date'])\n",
    "\n",
    "# grouping by month and getting the average coverage for each topic by month/source\n",
    "monthly_topic_coverage =articles_with_topics.groupby([ pd.Grouper(key='pubdate', freq='M'),'source']).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48d20d-e742-4fcb-882a-6782c02a83e0",
   "metadata": {},
   "source": [
    "Here's what the data looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab36851-5618-4f6a-a8cb-ca0e76581236",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_topic_coverage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886239f2-f1ed-4829-bf1c-9f4e47eca929",
   "metadata": {},
   "source": [
    "Now I can make a line plot with publication month on the x-axis and average topic proportion on the y-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a05e95-b157-4db0-95f3-d88899de5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.lineplot(monthly_topic_coverage, x='pubdate', y='topic 7', hue='source')\n",
    "plt.title('Abortion coverage by source');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3bb49-3fc2-42a3-bdff-0286d462098b",
   "metadata": {},
   "source": [
    "# Plotting Documents by topic\n",
    "\n",
    "Finally, I might want to visualize the entire corpus in a scatter plot. I'll use `TSNE` to reduce the dimensionality of my document-topic distribution from 15 columns down to just 2, then I can use these as the x and y coordinates in a scatter plot. I'll also color-code the results by topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a7a81-6beb-4683-bd2f-3fa38fd19fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "topics =topic_memberships.iloc[:,:15]\n",
    "tsne = TSNE(random_state=999, perplexity=30, early_exaggeration=120)\n",
    "embedding = tsne.fit_transform(topics)\n",
    "embedding = pd.DataFrame(embedding, columns=['x','y'])\n",
    "embedding['max_topic'] =np.array(topics).argmax(axis=1) # getting the topic most strongly associated with each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1184c-579a-4be7-b234-6b04420cd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_information = pd.concat([articles_with_topics, embedding], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726dbd1-c34a-47e7-9dd9-d56ebed36771",
   "metadata": {},
   "source": [
    "Now i'll make the scatter plot with Bokeh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f5efb-9384-44cd-8518-0fedb9980868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show,  output_file, save\n",
    "from bokeh.models import  ColorBar, LinearColorMapper, CrosshairTool, Span, BasicTicker\n",
    "from bokeh.transform import transform\n",
    "import bokeh.palettes\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7213b3d-a80c-4a11-b10e-f4bd114d05e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90390001-42f6-4655-85c9-a294d6c6d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary to map key-words to topics\n",
    "topic_dictionary = dict(zip(topics_labeled['topic'].astype(int), topics_labeled['keywords']))\n",
    "\n",
    "topic_information['topic_labels'] = topic_information['max_topic'].map(topic_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1a593-2d57-4428-84fb-269e7583d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making a pallette to map colors to topics:\n",
    "topic_colors = bokeh.palettes.d3['Category20b'][15]\n",
    "color_dictionary = dict(zip(range(15), topic_colors))\n",
    "topic_information['color'] = topic_information['max_topic'].map(color_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ab70d-2e2e-4386-a645-8ec2fbbb0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized HTML tooltip. The parts with an @colname will be filled in with data from my data frame.\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"width:400px;\">\n",
    "        <div>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 17px; font-weight: bold;\">@headline</span>\n",
    "                <div>\n",
    "                    <span>@date</span>\n",
    "                </div>\n",
    "            <br>\n",
    "        </div>\n",
    "        \n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; color: #966;\"><strong>Topic:</strong> @topic_labels</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px\"><strong>Source:</strong> @source</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 15px;\">Location</span>\n",
    "            <span style=\"font-size: 10px; color: #696;\">($x, $y)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "p = figure(title=\"Fox and CNN articles\",\n",
    "           tooltips=TOOLTIPS,\n",
    "           x_range=(min(topic_information.x)-1, max(topic_information.x)+1),\n",
    "           y_range=(min(topic_information.y)-1, max(topic_information.y)+1),            \n",
    "           width=2000, height=900,   \n",
    "           x_axis_label=\"Dim 1\",\n",
    "           y_axis_label=\"Dim 2\",\n",
    "           toolbar_location='above') \n",
    "                                                        \n",
    "\n",
    "# loop through each unique cluster in order. Doing this allows us to have an interactive legend on the plot\n",
    "for i in topic_information.max_topic.sort_values().unique():\n",
    "    data = topic_information[topic_information['max_topic'] == i]\n",
    "    topic_label = topic_dictionary[i]\n",
    "    p.scatter(x='x', y='y',  \n",
    "             source=data,   \n",
    "             legend_label = topic_label,\n",
    "             #fill_color = 'cluster_color',\n",
    "              marker = 'circle',\n",
    "             color = 'color',\n",
    "             \n",
    "             line_color = 'black',\n",
    "             alpha =.8,\n",
    "             size=12)\n",
    "\n",
    "p.legend.label_text_font_size = '20pt'\n",
    "\n",
    "p.legend.click_policy=\"hide\"\n",
    "p.legend.location = \"top_right\"\n",
    "p.legend.label_standoff = 30\n",
    "p.add_layout(p.legend[0], 'right')\n",
    "\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

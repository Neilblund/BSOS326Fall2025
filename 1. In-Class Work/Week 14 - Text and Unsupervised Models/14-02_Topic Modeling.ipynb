{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ae41cb-e984-49f9-a885-3c2b70191915",
   "metadata": {},
   "source": [
    "# Text analysis Pt II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e7d4e-1a51-4ae0-90b3-92257b36dd2f",
   "metadata": {},
   "source": [
    "We previously looked at using dictionary and machine learning tools to conduct sentiment analysis. In this class, we'll continue our discussion of text analysis by looking at some more exploratory methods for describing or understanding text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9810a9-5e75-42c2-913c-3e3ef2b56c4b",
   "metadata": {},
   "source": [
    "To start, install (or at least attempt to install) these packages first. Then restart the kernel. We'll use them later in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81d222-1918-4efb-83f2-724f12e31c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyLDAvis\n",
    "%pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231973-2244-4a11-b880-4871be3a06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis import lda_model\n",
    "from IPython.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ebb54-5541-4fb3-9b4c-9c0f03429a9a",
   "metadata": {},
   "source": [
    "We'll start by reading in some example data. This .csv contains articles scraped from CNN and Fox News from around 2020 through 2023:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078f3df-3dd1-4321-aa43-e507ed2441c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv('https://github.com/Neilblund/APAN/raw/main/news_sample.csv')\n",
    "# converting date to datetime\n",
    "articles['date'] = pd.to_datetime(articles['date'])\n",
    "\n",
    "# stripping some excess whitespace\n",
    "articles['headline'] = articles.headline.str.strip()\n",
    "articles['text'] = articles.text.str.strip()\n",
    "# creating a hyperlink for later use\n",
    "articles['hyperlink']=articles.apply(axis=1, func = lambda x: f'<a href={x.url}>{x.headline}</a>')\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ed706-cad5-4376-9a04-0cea9af21fe2",
   "metadata": {},
   "source": [
    "The articles in this collection are equally sized random samples from each source and each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2276d7-f73c-43b1-be10-a39ae0551313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(articles['source'], articles['year(date)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da15d6-dd2b-4167-bc5c-ac6eaba94da8",
   "metadata": {},
   "source": [
    "...although there are some disparities in the amount of coverage within each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250f4cd-dedb-4523-a74d-ab904aa1acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_counts = articles.groupby([pd.Grouper(key='date', freq='W'), 'source'])['url'].size().reset_index()\n",
    "weekly_counts = weekly_counts.pivot_table(index='date', columns ='source', values ='url')\n",
    "full_week_range = pd.date_range(start=min(articles.date).to_period('W').start_time,\n",
    "                                end=max(articles.date).to_period('W').start_time,\n",
    "                                freq='W')\n",
    "\n",
    "weekly_counts = weekly_counts.reindex(full_week_range, fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e760d5-f639-47ff-bf32-348a9a982bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_counts.plot()\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Weekly stories\")\n",
    "# Relocate the legend to the outside right of the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(.4, 1.2))\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f8d28-3351-49e1-a06a-d3752a5e9492",
   "metadata": {},
   "source": [
    "Our goal in this analysis will be to compare and contrasts the contents of these texts. Note here that we do have some additional data about each article beyond the text: the source and the headline, but the most interesting part is the text of each article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc187b-493b-4701-93ac-dda5bedf75f7",
   "metadata": {},
   "source": [
    "# Pre-processing data\n",
    "Just like in the previous class, we'll start by doing some initial pre-processing to our texts to make them into a document-term matrix. Our steps here will be more-or-less the same ones we used for the supervised modeling:\n",
    "\n",
    " - Splitting text into individual words\n",
    " - Lower casing and removing stop words\n",
    " - Stemming to remove endings like -ed or -ing\n",
    " - Creating a Document-Term-Matrix with one column per word and one row per document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe175a8a-dd69-42bb-ae73-3c0edbf2cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token).lower() for token in tokens if token not in eng_stopwords and token.isalpha()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )\n",
    "\n",
    "\n",
    "dfm = vectorizer.fit_transform(articles['text'])\n",
    "\n",
    "# get the names of the features for future use\n",
    "features = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476c559-997a-4fa1-b2ae-df957f0cabe2",
   "metadata": {},
   "source": [
    "The `dfm` object we created is a sparse matrix. By using the `.toarray()` method on this object, we can convert it to a numpy array and then use methods like `sum`, `sort`, and `where` to manipulate the results. For instance, if we wanted to calculate the number of occurrences of each term, we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74ff5b-650b-4003-b004-13cb9e10fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4741a3-5647-42c3-8d2a-f262dd4cd7a1",
   "metadata": {},
   "source": [
    "Or we could sort this in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0228f3-4a66-458b-80ee-f22e824b39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(dfm.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185b85f-96c5-47f8-ae1a-b776dc8c5e0c",
   "metadata": {},
   "source": [
    "Or, we could use `argsort` to get the indices that will sort the array from highest to lowest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7389d-362f-4d0d-9a07-784f30cfdfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(dfm.toarray().sum(axis=0))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d23ce-a140-437e-99f5-9d68ee58a576",
   "metadata": {},
   "source": [
    "Or we could sort the `features` (the terms used to build the document-feature matrix) in order of their occurrence from most frequent to least frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783401e-a6a0-48c1-ad97-a9dd0c9061b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "indices = np.argsort(dfm.toarray().sum(axis=0))[::-1]\n",
    "features[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa17671-cd9d-400c-a88b-7852146e555d",
   "metadata": {},
   "source": [
    "Finally, we could use the `where` method on the original data frame to get the indices of documents that come from a particular source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab72cc-1c3e-490b-9a41-cbe0fee8f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(articles.source==\"Fox News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084a696-7cd1-49bc-b865-83308bd7f438",
   "metadata": {},
   "source": [
    "And we could put all of this together to do something like identify the most common terms across all articles in Fox News stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60332e48-3df4-41c4-8cf5-15afffbb3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_articles = np.where(articles.source==\"Fox News\")\n",
    "fox_dfm = dfm.toarray()[fox_articles]\n",
    "indices = np.argsort(fox_dfm.sum(axis=0))[::-1]\n",
    "features[indices][:10].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c9aa4-0b15-440e-bfb6-88f0a37d7e7f",
   "metadata": {},
   "source": [
    "## Descriptive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33104a10-2bb7-4305-bc05-833489c3136e",
   "metadata": {},
   "source": [
    "We can start with a couple of simple descriptive and comparative visualizations for each text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13a403-4963-4894-bd88-9be543d41ab2",
   "metadata": {},
   "source": [
    "One simple exploratory approach might be to look at what terms are more strongly associated with Fox News compared to CNN.\n",
    "\n",
    "The `calcKeyness` function is a custom function included in the `text_functions.py` file in this directory. It gives us a way to compare term frequencies from two different sources using their relative ($log_2$) odds ratios. \n",
    "\n",
    "The general usage will be something like: `calcKeyness(X, y)` where `X` is the sparse matrix that we get from using `CountVectorizer` and `y` is a boolean vector that equals False for the \"baseline\" category and \"True\" for the category we want to compare against. \n",
    "The code below is going to calculate the odds ratio for terms that appear in CNN articles (positive values) compared to terms that show up more often in Fox News articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fa743-ebab-4cae-952f-8ae4a80465c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_functions import calcKeyness\n",
    "keyterms = calcKeyness(X=dfm,                         # the document-term matrix\n",
    "                       targets = articles['source'] == \"CNN\", # True if the article is from CNN (False for Fox News)\n",
    "                       minimum_threshold=200,       # remove words that occur less than 200 times\n",
    "                       feature_names=features)      # including the vocabuly so we have labels for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a198d11-9ebd-4096-9896-422cd1334308",
   "metadata": {},
   "source": [
    "The negative values are terms more strongly associated with Fox News articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889ddd7-dee4-4024-a30d-df16079e8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0ddca-4595-4d7b-859d-16724f57df6e",
   "metadata": {},
   "source": [
    "Terms with a positive odds ratio are more strongly associated with CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc48764-e549-4f7a-b5ea-0d2304e65e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms.tail(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fbe00-7ec6-4319-b6cf-118097d33f26",
   "metadata": {},
   "source": [
    "A visualization can also be helpful for getting a sense of these results. If you were following the news in 2020, you might be able to spot some terms associated with stories that were in the news around that time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063bb0b-52fc-4358-a805-4a64e67fb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bottom = pd.concat([keyterms.iloc[:15], keyterms.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'term',    \n",
    "                 hue='term',\n",
    "                x=top_bottom['oddsratio'],dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Term associations Fox News (negative values) vs. CNN (positive values)', ylabel='term')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f59d8-23e4-4336-8325-c4e41ed1fb26",
   "metadata": {},
   "source": [
    "### Wordclouds \n",
    "We can also make a word cloud, either for the entire corpus, or separately for each source.\n",
    "\n",
    "(wordclouds are not necessarily a great way to visualize text, but they look cool and people like them, so there's something to be said for playing the hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bcd18-c6d3-40f5-a313-63865cd3a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud().generate_from_frequencies(dict(zip(vectorizer.get_feature_names_out(), dfm.toarray().sum(axis=0))))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694e381-058d-484f-a841-533f6a940c8b",
   "metadata": {},
   "source": [
    "Or, we can compare CNN to Fox side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426b342-f89a-4e09-bde5-0faf0fc0c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "cnn_dfm = dfm[np.where(articles['source'] == \"CNN\")]\n",
    "\n",
    "cnn_wordcloud = WordCloud().generate_from_frequencies(dict(zip(vectorizer.get_feature_names_out(), cnn_dfm.toarray().sum(axis=0))))\n",
    "axes[0].imshow(cnn_wordcloud, interpolation='bilinear')\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"CNN\")\n",
    "\n",
    "fox_dfm = dfm[np.where(articles['source'] == \"Fox News\")]\n",
    "fox_wordcloud = WordCloud().generate_from_frequencies(dict(zip(vectorizer.get_feature_names_out(), fox_dfm.toarray().sum(axis=0))))\n",
    "axes[1].imshow(fox_wordcloud, interpolation='bilinear')\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Fox News\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02070ea-7929-4a39-ba80-122ce1996f3b",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "We know that terms like \"migrant\", \"immigrant\", \"border\" etc. probably all reference the same general idea, but our bag-of-words method fails to capture even fairly obvious relationships like this. Topic modeling is a way to identify important themes or ideas in texts that can pick up on some of these conceptual similarities.\n",
    "\n",
    "We'll use **Latent Dirichlet Allocation** (LDA), a common method for topic modeling, to these texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b70d2-3bb5-4fe6-b808-375950b73aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "LDA is a statistical model that identifies topics by taking advantage of the fact that related words tend to appear together in the same documents. LDA is an example of an **unsupervised machine learning model**: we don't have any labels, we just have a collection of documents and a rough idea of the number of topics, and the model will infer the rest for us.\n",
    "\n",
    "\n",
    "### Dirichlet distributions\n",
    "\n",
    "Before starting, its useful to have a rough idea of what we mean when we talk about a \"latent dirichlet\". A dirichlet distribution is a probability distribution that can be used to model the probabilities of multivariate outcomes. For instance, you might think about the probabilities of attempting to manufacture some fair six-sided dice. For each die you make, the total probability of all six faces should sum to one, and every side will have a positive probability that's greater than zero. However, sometimes you might have manufacturing defects that cause one side to be a little more likely than the others. We could use a dirichlet distribution to model this hypothetical process for manufacturing a single dice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279e97-02f6-4eb7-ad30-025d99746484",
   "metadata": {},
   "outputs": [],
   "source": [
    "sides  = 6     # the number of outcomes\n",
    "n = 1      # the number of random draws from the dirichlet\n",
    "alpha = [100] * sides  # the concentration parameter (higher values = more even distribution, lower values = more unbalanced distribution)\n",
    "# manufacturing a single die:\n",
    "pd.DataFrame(np.random.dirichlet(alpha, n), columns=range(1, len(alpha)+1 ,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82883d99-21c2-4839-bdc2-91716f6e20da",
   "metadata": {},
   "source": [
    "You can try running the code above with different values for `alpha`, or simulate multiple dice by increasing the value of `n`, or model dice with more faces by increasing `sides`. Take note of how different parameters impact the values of your randomly generated \"dice\". You should note that using higher values of alpha usually results in a more uniform distribution of probabilities across all six faces, while lower values will typically result in a more uneven distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb581197-47a5-4975-8d79-bb908b6c99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [.1] * sides  \n",
    "# manufacturing a single die:\n",
    "pd.DataFrame(np.random.dirichlet(alpha, n), columns=range(1, len(alpha)+1 ,1)).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c66daf-1ee9-4f78-b601-b308d228c7db",
   "metadata": {},
   "source": [
    "Increasing the value of `sides` will just increase the number of possible outcomes. So we could model a 20-sided die by increasing that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a537118-136a-4b4d-b588-1cfe3b5a89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sides  = 20 \n",
    "n = 1      \n",
    "alpha = [100] * sides \n",
    "pd.DataFrame(np.random.dirichlet(alpha, n), columns=range(1, len(alpha)+1 ,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073d9a5-b05c-4e17-843b-d024770f8e72",
   "metadata": {},
   "source": [
    "In LDA, we assume that documents are generated by sampling from a pair of these dirichlet distributions:\n",
    "\n",
    "- A **Topic-word distribution** (sometimes called phi or beta)  models the probability of any word occuring in a single topic. A topic related to Covid-19 might have a high probability of terms like \"Fauci\", \"mask\", or vaccine\". A topic related to the election might have a high probabiltiy of \"vote\", \"caucus\" or \"turnout\"\n",
    "- A **Document-Topic distributions** (theta) models the probability of drawing one of `K` topics within a given document. So a document might be drawn 75% from the Covid-19 topic, 20% from the election topic, and 5% from some other random topic.\n",
    "\n",
    "So, we might have something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd598d16-a189-413e-a5d9-c2c80c8e1d74",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <div style=\"flex: 1; padding-right: 10px;\">\n",
    "        <!-- Content for the first column -->\n",
    "        <h3>Topic 1</h3>\n",
    "<table style=\"border-collapse:collapse;border-spacing:0\" class=\"tg\"><thead><tr><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Term</th><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Prob.</th></tr></thead><tbody><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">covid</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.04</td></tr>\n",
    "<tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Fauci</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.02</td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">vaccine</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.01</td></tr>\n",
    "<tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">mask</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.01</td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">...</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\"></td></tr>\n",
    "<tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">election</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.0001</td></tr></tbody></table>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; padding-left: 10px;\">\n",
    "        <!-- Content for the second column -->\n",
    "        <h3>Topic 2</h3>\n",
    "        <table style=\"border-collapse:collapse;border-spacing:0\" class=\"tg\"><thead><tr><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Term</th><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Prob</th></tr></thead><tbody><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">vote</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.03</td></tr>\n",
    "<tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">caucus</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.02</td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">turnout</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.01</td></tr>\n",
    "<td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">poll</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.01</td></tr>\n",
    "\n",
    "<tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">...</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\"></td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">covid</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.0001</td></tr></tbody></table>\n",
    "    </div>\n",
    "        <div style=\"flex: 5; padding-left: 10px;\">\n",
    "        <!-- Content for the second column -->\n",
    "        <h3>Document distribution</h3>\n",
    "        <table style=\"border-collapse:collapse;border-spacing:0\" class=\"tg\"><thead><tr><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\"></th><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Topic 1</th><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Topic 2</th><th style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Topic 3</th></tr></thead>\n",
    "<tbody><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Document 1</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.75</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.20</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">.05</td></tr></tbody></table>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ae89e-0974-4b32-8631-64cc784ba50c",
   "metadata": {},
   "source": [
    "\n",
    "LDA assumes a generative model where each article is written by randomly sampling a topic from a document's topic distribution, and then randomly sampling a word from the selected topic distribution. So I might write a document where I drew 75% of my words from a \"Covid-topic\" and 20% from a \"election topic\" and another 5% from some other random topic. Obviously, no one actually writes documents by randomly sampling words from a list. The generative model is mostly useful because it gives us a clear idea of what we need to optimize for in our machine learning model: we want to find the values of $\\phi$ and $\\theta$ that would maximize the likelihood of generating the collection of documents we have in our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022a7d1-6db3-446b-86ea-8a01b0d6f5c6",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48699a9a-bd9d-456d-8640-868ba6e20b6e",
   "metadata": {},
   "source": [
    "Let's try fitting an LDA model. \n",
    "\n",
    "Similar to the clusters in K-means clustering, LDA doesn't choose the number of topics for us automatically. We'll have to make a guess at that based on our intuitions. For this model, I'l set `k=15`. This is probably too low - it's common to see 20, 40 or even 100 or more topics for large-scale models - but we'll use it here because a lower value will make the model converge more quickly.\n",
    "\n",
    "We'll also want to set a `random_state` value. Multiple LDA models with the same number of topics and the same data should find similar topics, but even then the ordering of those topics is arbitrary, so setting the `random_state` argument will ensure that we can replicate our results.\n",
    "\n",
    "We'll create an LDA model, then put it in a pipeline along with our function for converting texts into a document-term-matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdfe14-efa5-46fc-aeeb-fbe4afb79424",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "ldamodel = LatentDirichletAllocation(n_components = k, # number of topics. Try different numbers here to see what works best. Usually somewhere between 20 - 100\n",
    "                                random_state = 123, # random number seed. You can use any number here, but its important to include so you can replicate analysis\n",
    "                                doc_topic_prior = .01,\n",
    "                                topic_word_prior = .001\n",
    "                               ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0ead5-b43e-455c-a592-e1f49ebafeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )\n",
    "\n",
    "\n",
    "pipeline_steps = [\n",
    "    ('vectorizer', text_vectorizer),\n",
    "    ('lda', ldamodel )\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "lda_pipeline = Pipeline(pipeline_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b16c8b-bc24-418e-a2f7-dbc4c44aa3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = lda_pipeline.fit_transform(articles['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706de9c3-d65d-4dbb-a68c-cf4dcf4b1704",
   "metadata": {},
   "source": [
    "The `doctopic` object has the document topic probabilities in an array. So here's the breakdown of topics in document 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee26b1b-e0f9-46c9-b52b-9c7eb792db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd05e4-de44-4486-b1ba-19a8a838fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(k), doctopic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f4abd-35c0-4428-9ffd-9b9f8b3fea61",
   "metadata": {},
   "source": [
    "The term distribution for each topic is stored in the `.components_` attribute of the fitted LDA model. So here's some top terms from topic 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1aaddf-beac-4ad7-96c9-e7e9d1f0346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the features from the vectorizer part of the pipeline (this is just the vocabulary from the texts)\n",
    "features = lda_pipeline['vectorizer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e5219-ca43-4623-a19b-c5779a423f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[np.argsort(lda_pipeline['lda'].components_[0])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84634a3b-6172-46b0-8de7-fd597c262d8e",
   "metadata": {},
   "source": [
    "For ease of use, we can put these together in a Pandas Dataframe with named columns (we'll also normalize the rows so that each sums to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b31b44-d8ee-417d-84f1-568a909c226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_frame = pd.DataFrame(lda_pipeline['lda'].components_, columns=features)\n",
    "phi_frame = phi_frame.div(phi_frame.sum(axis=1), axis=0)\n",
    "phi_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ab8b1-90da-4c46-aae2-b4ce58c2be43",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "A lot of the complexity of using LDA lies in intepreting the topics themselves. The simplest way to do this is by looking at the most probable words for each topic, which we can do by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424e8ab-94ae-4df8-b303-298947451f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top keywords in each topic\n",
    "\n",
    "n_terms = 10\n",
    "ls_keywords = []\n",
    "ls_freqs = []\n",
    "topic_id = []\n",
    "    \n",
    "for i,topic in enumerate(lda_pipeline['lda'].components_):\n",
    "     # Sorting and finding top keywords\n",
    "    word_idx = np.argsort(topic)[::-1][:n_terms]\n",
    "    freqs = list(np.sort(topic)[::-1][:n_terms])\n",
    "    keywords = [features[i] for i in word_idx]\n",
    "        \n",
    "        # Saving keywords and frequencies for later\n",
    "    ls_keywords = ls_keywords + keywords\n",
    "    ls_freqs = ls_freqs + freqs\n",
    "    topic_id = topic_id + [i] * n_terms\n",
    "        \n",
    "    \n",
    "        # Printing top keywords for each topic\n",
    "    print(i, ', '.join(keywords))\n",
    "top_words_df = pd.DataFrame({'keywords':ls_keywords, 'frequency':ls_freqs, 'topic_id':topic_id})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36ecf7-ec99-468a-abf8-5439ad0d386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040055c8-aa77-49b6-8f08-c5f5c6ea2996",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 1: Wrap the code above in a function that takes `n_terms`, a fitted `lda` model, and a list of `features` as arguments and returns a data frame with the top n terms for each topic in descending order of frequency. Try running the same function with a few different values for `n_terms`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d235e3-60ae-4ac8-99c6-b5f9c1efbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2710c-a995-415c-b509-fad75ea4a32a",
   "metadata": {},
   "source": [
    "We can also plot terms by frequency within each topic (although this may get unwieldy for models with a larger number of components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20514c9e-fd24-492b-a66d-a5fbce73746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(top_words_df, x = 'frequency', y = 'keywords', col = 'topic_id', kind = 'bar', sharey = False, col_wrap=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7202ba-2d61-453e-9321-d10ee3353b94",
   "metadata": {},
   "source": [
    "In many cases, an interactive visualization can make it easier to identify topics. The LDAvis package provides an easy way to create an interactive HTML file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f94c-e284-4691-b82e-a2799620a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.lda_model.prepare(lda_pipeline['lda'], dfm, lda_pipeline['vectorizer'], mds='tsne', sort_topics=False, n_jobs = -1)\n",
    "word_info = panel.topic_info\n",
    "\n",
    "#To save panel in html\n",
    "pyLDAvis.save_html(panel, 'panel.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a2080-5b9e-4f2c-bd60-b73d1ffe32c2",
   "metadata": {},
   "source": [
    "In the left panel of the display below, you can see each topic scaled by its overall frequency in the corpus. The relative positions of each topic indicates how distinct they are, so that topics that are further apart should share fewer terms. The plot on the right will display top terms for each topic. Instead of using the probability of each term, the displayed in this visualization are ranked according to a metric that accounts for how specific each term is to each topic. In some cases, this can be a better way of identifying the concept each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723f748-aa2e-4288-adb8-a2f3dce85682",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('panel.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99b451-f576-4892-aa72-afb5b4acd9c8",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 2: Using the information we've gathered so far, see if you can assign a short label to each topic in the LDA model. Replace the generic labels in `label_map` below with some descriptive topic IDs and then recreate the catplot object from the previous section</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51678fb5-5758-4934-9c9b-54d9688bd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'topic 1',\n",
    "    1: 'topic 2',\n",
    "    2: 'topic 3',\n",
    "    3: 'topic 4',\n",
    "    4: 'topic 5',\n",
    "    5: 'topic 6',\n",
    "    6: 'topic 7',\n",
    "    7: 'topic 8',\n",
    "    8: 'topic 9',\n",
    "    9: 'topic 10',\n",
    "    10: 'topic 11',\n",
    "    11: 'topic 12',\n",
    "    12: 'topic 13',\n",
    "    13: 'topic 14',\n",
    "    14: 'topic 15',\n",
    "}\n",
    "# map the labels\n",
    "top_words_df['topic_label'] = top_words_df['topic_id'].map(label_map)\n",
    "\n",
    "# recreate the catplot object:\n",
    "sns.catplot(top_words_df, x = 'frequency', y = 'keywords', col = 'topic_label', kind = 'bar', sharey = False, col_wrap=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189cd2-086b-47a5-8770-8e89b7bce3d4",
   "metadata": {},
   "source": [
    "## Identifying Document Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19c5d3-dc21-4f15-b368-dcad61d18698",
   "metadata": {},
   "source": [
    "Remember that LDA gives us two distributions: a distribution for word occurrences in each topic, and a distribution of topic occurrences within each document. So we also have a way to see what documents are associated with what sources or what time periods. We just need to link the topic memberships in `doctopic` back to the original documents so that we can see which documents are getting categorized into which topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070fd35-6487-4343-b282-4f10f2ab00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_memberships = pd.DataFrame(doctopic)\n",
    "topic_memberships.columns = [\"topic \" + str(i)  for i in topic_memberships.columns ]\n",
    "topic_memberships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55baf222-a54b-47a8-b830-f158760b2cb3",
   "metadata": {},
   "source": [
    "Each row in this result represents one of the documents from our original data, and each column represents a topic. We can make things a little easier to interpret by appending some information about each article as additional columns onto this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645f87c-2498-4e89-ab49-d68991383ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_memberships['text'] = articles.text\n",
    "topic_memberships['source'] = articles.source\n",
    "topic_memberships['headline'] =articles.headline\n",
    "topic_memberships['url'] = articles.url\n",
    "topic_memberships['hyperlink'] = articles.hyperlink\n",
    "\n",
    "topic_memberships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203533a-642f-4254-9b96-f5b56b8ae6e0",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 3: Identify the top 5 articles most strongly associated with topic 6</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae50134-9b3f-4253-b4f7-230345add0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09033833-8832-4f84-a5eb-9a450db8583e",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 4: Which topics, if any, occurred in a higher proportion of Fox News stories vs. CNN?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8603a-be9c-4b82-b8df-674d96cc4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96b71-bafb-47e2-aca7-60b0f35ab19b",
   "metadata": {},
   "source": [
    "# Making a styled table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f151d-67c1-48b2-aa45-13bf46c70a2e",
   "metadata": {},
   "source": [
    "Since we included a hyperlink and an article title in our original data frame, we can make a styled table that includes a formatted link for the topic articles in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4195a-bd94-4d11-b8e2-327285887d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 10\n",
    "n_docs = 3\n",
    "top_documents = []\n",
    "top_index = topic_memberships.columns.values.tolist()[:15]\n",
    "for i, label in enumerate(top_index):\n",
    "    top_n_documents =  topic_memberships.sort_values(label, ascending=False).head()\n",
    "    terms={ 'topic' : i,\n",
    "           'mean proportion' : np.mean(topic_memberships[label]),\n",
    "        'docs' : '<br>'.join([i for i in top_n_documents['hyperlink'].to_list()[:n_docs]]),\n",
    "        'terms' : ', '.join([features[j] for j in np.argsort(lda_pipeline['lda'].components_[i])[::-1][:n_terms]]) \n",
    "    }\n",
    "    top_documents.append(terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076abba-70f8-4e0c-9037-cde210720ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_table = pd.DataFrame(top_documents).sort_values(['mean proportion'], ascending=False).reset_index(drop=True).style\n",
    "\n",
    "stylized_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412fb79-7ee9-4edd-98dc-3b372f6602ae",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 5: Try re-running the model with a larger number of topics. Compare your results. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3565f-9228-4a2e-92a2-8ab85e82fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c042f8-dd1d-4564-9200-d6df3580101d",
   "metadata": {},
   "source": [
    "## Visualizing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf538c2-5307-4e40-b090-e770eaf96616",
   "metadata": {},
   "source": [
    "The document-topic matrix gives us a lower-dimensional way to represent the contents of our different documents. If we take this matrix and perform some additional dimensionality reduction, we can use the results to visualize all of the texts in a 2-dimensional space. We'll use the T-stochastic nearest neighbor embedding model to get a 2-d representation of our documents. T-SNE is a dimensionality reduction technique like principal components analysis, but it tends to be more effective in cases where there are non-linear relationships between observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8541b-d03b-4e45-a5ae-54b63522a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Create a t-SNE model with 2 components\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_tsne = tsne.fit_transform(doctopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e42a0e-9946-427f-9dfe-593de0188899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the 2-d embedding to our articles data frame\n",
    "positions = pd.DataFrame(X_tsne, columns=['Dim 1', 'Dim 2'])\n",
    "positions = pd.concat([positions, articles], axis=1)\n",
    "positions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f544015-c184-4fb2-a035-d47a6b1c8f56",
   "metadata": {},
   "source": [
    "Creating some topic labels from the top words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d853f38-f648-49d6-9f69-fbfc2d3c7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = top_words_df.groupby('topic_id')['keywords'].agg(lambda x: ', '.join(x)).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb310747-c68f-48ce-9a5e-e9729c02747c",
   "metadata": {},
   "source": [
    "Identifying the most probable topic for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb547d-3212-4fc1-a772-ead54db29b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic = pd.DataFrame(doctopic).idxmax(axis='columns')\n",
    "positions['max_topic'] = max_topic\n",
    "positions = pd.merge(positions, topic_labels, left_on ='max_topic', right_on ='topic_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc3bfd-73a8-4213-8744-7d7b24f4b551",
   "metadata": {},
   "source": [
    "Showing the results in plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2faaf8-8c26-481a-ace9-1303b9a98423",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bd794-5517-41d3-8c5c-f810c860ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc0119-34ef-482d-88e1-56273d054b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(positions, x='Dim 1', y='Dim 2', \n",
    "                 title='Document positions from topic model',\n",
    "                width= 1800, height=800, color='keywords'\n",
    "\n",
    "                )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0660a349-b3ef-4082-b7a9-b05c5048461d",
   "metadata": {},
   "source": [
    "# Text analysis and sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98587fd-260d-4fa8-8b69-1d57efcb3eb3",
   "metadata": {},
   "source": [
    "You may need to install `nltk`, if you haven't done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c574a-d708-4ab1-b5b1-ab2d0dd90476",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf830732-d580-4896-b93d-9d16cc06f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer, LancasterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# included so I can add latex code\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81de662-0be5-437f-b01f-27b988af33b0",
   "metadata": {},
   "source": [
    "You'll also need to download some additional functions and data by running this code. (you'll generally only need to do this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307105e-7b18-41f6-aeab-17c2a561c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e9a8-5b58-4ff8-bf47-c6ef2776b004",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "The purpose of **text analysis** is to extract meaning from text data. This involves cleaning and processing text data, as well as using analysis methods that are able to get something **quantitative** out of something that doesn't inherently have **numbers**. \n",
    "\n",
    "One way to extract meaning from text is by assigning values of **sentiment**. The words we use have meaning, and we can assign measures of what they are intended to portray. For example, the word \"bad\" is generally a negative sentiment (slang usage notwithstanding), while \"good\" has a positive sentiment. The word \"hurt\" generally also has a negative sentiment, while \"heal\" has a positive one. In this way, we can attempt to put different words onto the same scale and measure the overall sentiment of text.\n",
    "\n",
    "In this section, we will look at doing a type of analysis called **sentiment analysis**, which is a class of techniques designed to extract this type of meaning from text data. We'll compare two different approaches: one based on a \"dictionary\" method, and another that relies on a machine learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a19ed5-36da-452b-be0c-24700b235f2f",
   "metadata": {},
   "source": [
    "## Dictionary based sentiment analysis\n",
    "\n",
    "VADER is a **dictionary-based** method, meaning it is pre-built and comes with a list of words and scores. To use it, we need to download the list of words with scores, then apply those scores to the words within our documents. Combining the scores of the words/tokens within our document gives us the overall sentiment of the document. For VADER, we will get back the negative, neutral, positive, and compound scores of a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71398e07-1533-4d99-9684-dbad39a8b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"I'm very angry!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947f85f-5228-42be-8305-70f4539ab4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"Feelin fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bfb8f-5b51-4e70-9124-5ba47fda338d",
   "metadata": {},
   "source": [
    "The scores for negative, neutral, and positive are always positive, and indicate how much of that type of sentiment is present in the document. The compound score is a value from -1 to 1 that provides an overall summary of how positive or negative that document is in its sentiment. \n",
    "\n",
    "The compound score is most often used, and typically, the threshold for being considered positive, neutral, or negative is as follows:\n",
    "- positive sentiment: compound score >= 0.05\n",
    "- neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "- negative sentiment: compound score <= -0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cf8b4-1da3-41f5-a6be-89a5a3a33323",
   "metadata": {},
   "source": [
    "Vader is designed with social media in mind, so it will also pick up on things like slang and emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906414d-6579-49f5-ae61-d501109ca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\":-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2edef1-34ab-49f6-8293-02314bd46bf9",
   "metadata": {},
   "source": [
    "Moreover, it includes some rules that pick up on some forms of negation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea7a38-097d-4736-91f2-227158d82b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I'm not angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c7b86-76f8-4778-a1c1-cf2ac4b4dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I'm not not angry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c9006-7013-40b0-9346-796e010f992e",
   "metadata": {},
   "source": [
    "We can try out applying the VADER method to a sample of Tweets that's included as part of the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de85a7-e8a4-4574-a97e-2859ee8de98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = nltk.corpus.twitter_samples.strings()\n",
    "tweets[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf1b6e-2237-463a-8dbf-fd75c9eacf7e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;font-weight: bold;\">Question 1:</h2><p style=\"color:red;font-weight: bold;\">Get the sentiment scores for the <code>tweets</code> data. Create a histogram to show the distribution of positive and negative sentiments</p>\n",
    "<p><b>Hint</b>: Since the <code>polarity_scores</code> method requires a single string, you'll need to use a list comprehension to get the sentiment score for each tweet. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af053571-0dfd-4e6e-9716-48373116b8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05625684-c577-484a-ae64-1013ce69d186",
   "metadata": {},
   "source": [
    "## Evaluating the sentiment scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d7060-9f98-4213-b795-c00af4b7feca",
   "metadata": {},
   "source": [
    "It seems intuitive that a dictionary of positive and negative terms would be a good way to classify text, but how do we know whether this sentiment dictionary is any good? One way to test out the performance of the VADER model is by comparing its predictions to a \"ground truth\" source of evidence. The `imdb_reviews` dataset has two columns: the text of a user review and a label that is 1 if the user gave a positive rating (>=7) and its zero if the user gave a negative rating ( >=4). Since this corpus includes both text and labels, we can use it as a way to evaluate VADER against a data source with some known labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932d6e7-87bd-41bf-9384-5e3d068ec18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('imdb_reviews.csv').dropna()\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7923d8-2a3e-414b-8358-f4e2b42079aa",
   "metadata": {},
   "source": [
    "We'll need to convert the sentiment scores to a dichotomous variable to match the way sentiment is recorded in the IMDB corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e7cfd-0122-4642-86b1-9756fd2574d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity =  pd.DataFrame([sia.polarity_scores(i) for i in imdb['text']])\n",
    "\n",
    "# assign a positive or negative label based on the compound score: \n",
    "polarity['predicted_positive'] = polarity['compound']>=.05\n",
    "\n",
    "polarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f39332-afd9-48bf-949c-435f225e94f2",
   "metadata": {},
   "source": [
    "Then, we'll add the \"ground truth\" labels to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aae6a-a8e1-4e0a-8c4f-e0343cd0b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the \"ground truth\" to the polarity\n",
    "polarity['actual'] = imdb['label']\n",
    "# view the results:\n",
    "polarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210cf087-f708-43dd-a4aa-b4cc13416469",
   "metadata": {},
   "source": [
    "Finally, we'll make a confusion matrix and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6a759-a523-4ebe-94a8-1e4f29b175a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = pd.crosstab(polarity['predicted_positive'], polarity['actual'],  margins=True)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bfcb4-c058-4857-ba34-eee575656fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(polarity.actual, polarity.predicted_positive ,\n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(polarity.actual, polarity.predicted_positive))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(polarity.actual, polarity.predicted_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dca95-a3e2-4500-9f7c-53f0586ca843",
   "metadata": {},
   "source": [
    "Pretty good! Although VADER wasn't really developed for use in the particular context, it still does a decent job of identifying positive and negative reviews, although there are still plenty of documents that are not correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960202-2747-40dd-93bf-ab266470c0e8",
   "metadata": {},
   "source": [
    "# Machine learning for sentiment\n",
    "\n",
    "Overall, the VADER tool performed well, but still had plenty of errors. Can we improve on this performance? Maybe! Instead of using a lexicon to determine if the texts are positive or negative, we can make a machine learning model that infers the terms that are associated with positive or negative documents automatically. In general, even fairly simple machine learning models can beat lexicon-based methods. \n",
    "\n",
    "Recall that, for supervised learning, we must have a y variable that we know. That is, we need to have a the y variable in our dataset, so that we can build our model and use that model to predict y for future data. We're going to compare the predictions from the VADER sentiment lexicon to another widely used machine learning model called a Naive Bayes classifier. It works by using Bayes' Theorem to calculate the probability of each class ($C_k$ in the formula below) given a predictor $x_i$:\n",
    "\n",
    "$$ P(C_k|x_i) = \\frac{P(C_k) P(x_i|C_k)}{P(x_i)}$$\n",
    "\n",
    "- $P(x_i)$ is the probability of feature $x_i$ in all documents\n",
    "- $P(C_k)$ is the probability of class $k$\n",
    "- $P(x_i|C_k)$ is the probability of $x$ in documents with class $k$\n",
    "- $P(C_k|x_i)$ is the probability of class $k$ given feature $x_i$\n",
    "\n",
    "The model is called \"naive\" because it assumes (incorrectly!) that each word is conditionally independent of all other words given the class. This turns what might otherwise be a gnarly calculus problem into a bunch of division and mulitiplication: we just need to calculate the conditional probability of each class separately for each term, multiply these probabilities together, and then pick the class with the highest total probability. Despite this wild oversimplification of how words work, naive bayes classifiers have a pretty good track record for document classification problems like this one.\n",
    "\n",
    "\n",
    "## Splitting the data\n",
    "\n",
    "Before we do anything, we'll want to split the data into a training and testing set, just like we did with our other supervised models. We'll use `X_train` and `y_train` to train the model, and then we'll use `X_test` and `y_test` to evaluate on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd145a-17d3-4a82-a056-c5601a9c8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imdb.text, imdb.label,\n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8e720-de78-48ac-bfd8-754e2e55098a",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Our goal here is to apply a mathematical model to text data, so a basic step is going to be converting words into some kind of a sensible numeric format.\n",
    "\n",
    "The VADER model does a lot of the \"words-into-numbers\" stuff for us. But for the current case we're going to need to do that process ourselves. As a starting point, we'll use what's commonly called a \"bag-of-words\" representation of our texts. A \"bag-of-words\" model simplifies texts by ignoring things like word-ordering, parts of speech, tone, context etc. and instead just focuses on \"how many times a term occurs in a given document\". To convert our reviews to a bag-of-words, we're going to do the following pre-processing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb601e-20d6-4062-9250-91bfddaf2c17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Tokenization** splits texts into smaller units. In the current case, this will be individual words without punctuation. But it could also be sentences, paragraphs or \"n-gram\" (groups of 1, 2, 3... words)\n",
    "2. **Stopword Removal** removing common terms like \"a\", \"and\", \"the\" that are grammatically useful but uninformative for many text models\n",
    "3. **Normalization** combining terms that are more-or-less equivalent by doing things like converting words to lower-case or removing word endings through stemming or lemmatization\n",
    "\n",
    "\n",
    "(In some cases we may change the ordering of these steps, or we might do some cleaning and normalization and then decide, based on a closer look at our results, that we need to go back and do some additional cleaning. Our end goal, however, is generally to a representation of a text that is simple *enough* without sacrificing too much nuance or complexity.)\n",
    "\n",
    "Once we've made our bag-of-words, we'll use it to create what is called a \"document-term-matrix\" where each row represents a single document, each column represents a word that occurs anywhere in the entire collection of documents, and the cell values indicate a count of the number of times word `j` occurs in document `i`\n",
    "\n",
    "For example, if I think about the sentences as a collection of three documents:\n",
    "\n",
    ">    \"See Spot Run. Spot runs fast. Run spot run!\"\n",
    "\n",
    "\n",
    "I would represent those documents as a document-term-matrix that looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af7739-32c2-453c-852d-147448e1a075",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\"><thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\">See</th>\n",
    "    <th class=\"tg-0pky\">Spot</th>\n",
    "    <th class=\"tg-0pky\">Run</th>\n",
    "    <th class=\"tg-0pky\">Fast</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">See Spot run.</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Spot runs fast</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Run Spot run.</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Note that, for real-world text analysis, the number of columns is going to grow exponentially as we include more documents, and most cells are just going to have values of `0`. This tendency to get very large \"sparse\" matrices is a recurring problem in text analysis and we'll have to make a lot of simplifications to keep things manageable. Many of the processing steps below are intended to allow us to combine or drop columns from our document term matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104328e6-5f6a-4f5b-83c9-5da08834e083",
   "metadata": {},
   "source": [
    "The `scikit-learn` package has a `CountVectorizer` function that will let us do all of the necessary processing in one step, but just to get a sense of what each component does, we'll try each part separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f07b14-57eb-4edf-9fd6-01ee279abb50",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "\n",
    "In the tokenization step, we'll split up documents into individual words. To do this, we'll use the `word_tokenize` function from the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab076900-6b3b-4298-91e7-24b8438e3ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = 'See Spot run. Spot runs fast. Run Spot run!'\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f8055-7464-4e3e-a08b-38c6a5dc2f58",
   "metadata": {},
   "source": [
    "For the IMDB reviews, we're going to go a step further and lower case all of the texts. This will ensure that our document-term-matrix treats \"This\", \"this\", and \"THIS\" as a single word instead of creating separate columns for terms that mean the same thing, so our output will end up looking something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eac810-6a1c-491d-89f1-aab727214f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lower case and drop empty\n",
    "\n",
    "nltk.word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719f82a-5893-4f57-a2fc-6e783caf5f64",
   "metadata": {},
   "source": [
    "### 2. Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17a111-88dc-476f-a5e4-4ba8b8c8e90b",
   "metadata": {},
   "source": [
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic information. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with the `nltk` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea25547-2f8a-42d2-b6c7-404f7fe3f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb965b4-2b93-4256-aa92-f7065e40dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcd57b-035a-45f0-b296-f2c0555ae596",
   "metadata": {},
   "source": [
    "We'll convert this list of stopwords to a set (this will speed up some processing steps), and then we'll use it to filter the results from the tokenization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cde93a-cd38-41d8-8389-1036fd5dc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the stopword list and converting it to a list\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This text contains a couple of stopwords.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "filtered_tokens = [w for w in tokens if w not in  eng_stopwords]\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde506f3-5b58-468b-8162-521341568b04",
   "metadata": {},
   "source": [
    "While we're filtering, we might also want to remove punctuation marks from the bag of words. So we'll use `.isalpha()` to remove anything that isn't an a letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe8c25-3863-4f59-98c7-232d60c3a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [w for w in tokens if w not in  eng_stopwords and w.isalpha()]\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648dd45-7185-4698-83ae-a52ff94371d5",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00124386-92f4-46f0-88e6-09477c8dcbe1",
   "metadata": {},
   "source": [
    "Finally, we'll try to simplify our bag-of-words analysis by grouping together different inflections of the same terms. Word-endings that indicate things like pluralization and tense are useful in the context of human communication, but they're not informative when we're trying to do things like identify the topic or tone of a text.\n",
    "\n",
    "There are two common approaches to this kind of normalization: \n",
    "\n",
    "- **Lemmatization** uses parts-of-speech and context clues to convert words to their basic dictionary form. \n",
    "- **Stemming** uses some simple hueristics to remove word inflections. \n",
    "\n",
    "Stemming is more error-prone than lemmatization, and sometimes results in words that you won't find in a dictionary, but it has the advantage of being much faster because it relies in simple rules whereas lemmatization considers word context and parts-of-speech. \n",
    "\n",
    "There are multiple stemming algorithms with different rule sets and differing strengths and weaknesses. In this notebook, we'll use the Snowball Stemmer. You'll notice this works pretty well for many words, but gives odd results for others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741d976-42d3-4b7d-8184-a32773edd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stemming algorthim\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d3c6c-0f96-447a-9f19-5a5a4a77b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it to some terms\n",
    "forms = ['lying', 'fisherman', 'change', 'systematic', 'stapled', 'catlike', 'argument', 'alphabetical']\n",
    "print([stemmer.stem(i) for i in forms])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ece9f-a3f8-4cbd-80d4-c3cd57646d6d",
   "metadata": {},
   "source": [
    "You can contrast these results with the results from an alternate stemming algorithm, such as the LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fccf4-b1f1-4190-94b6-589cf407116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcb7fd-576b-4205-989d-7ece20d57b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lying', 'fisherman', 'change', 'systematic', 'stapled', 'catlike', 'argument', 'alphabetical']\n",
    "print([lancaster.stem(i) for i in forms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07bbc1-297a-4fbe-9579-b8fec17e58f7",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We'll wrap all three steps together in a single function that will take a document and return a processed bag of words. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db623e6c-30d0-4c13-b85c-86ac8bfc014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens if token not in eng_stopwords and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bbc49-eb1c-47cd-9abb-7431b2e719cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"This is a sentence that has been fully processed into a bag of words!! Isn't it great?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e7008-fdad-49de-9415-78a3e901f751",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building the Document-Term Matrix\n",
    "\n",
    "Finally, we'll use `CountVectorizer` to convert the tokenized texts into a document-term-matrix with counts for each term. Note that we're passing the `tokenize` function as one of the arguments to the `CountVectorizer` here, so the stop word removal and stemming will be automatically applied before we get our word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de4dc5-983c-423c-b3dd-6253fbfa8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f69276-23e1-4065-9f77-46903fdd4537",
   "metadata": {},
   "source": [
    "Here's what the resulting data would look like if we applied this onto some example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd24f4-54ed-4422-a647-68655ccfe06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Best movie I've ever seen\",\n",
    "             \"This one is a total flop\",\n",
    "             \"I give this one two thumbs up.\",\n",
    "             \"I almost walked out of the theatre.\"\n",
    "            ]\n",
    "dtm = vectorizer.fit_transform(sentences)\n",
    "pd.DataFrame.sparse.from_spmatrix(dtm, columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e430ee2-9628-4384-ad57-d4bcac541f64",
   "metadata": {},
   "source": [
    "## Building a pipeline\n",
    "\n",
    "No we've got everything we need to process the data for text analysis. We just need to add our naive bayes classifier. We'll use a `MultinomialNB()`, which is a variant ofthe naive bayes model that can include information about term frequency.\n",
    "\n",
    "To simplify some of the coding here, we'll use a `Pipeline` class that will combine the pre-processing and the classifier steps into a single function. We can specify a pipeline with a list of tuples that define each processing step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1942d-1631-44c0-b9a4-feebd60f15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "nb_pipeline = Pipeline([\n",
    "    ('DTM',CountVectorizer(\n",
    "                             ngram_range=(0,1), \n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1,\n",
    "                             min_df = .0025\n",
    "                            )),\n",
    "    ('naivebayes',MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed0511-751e-4d90-83f2-f3d356671a21",
   "metadata": {},
   "source": [
    "**Note**: In addition to stemming and stopword removal, you might notice we also set values for `max_df` and `min_df`. `max_df=0.1` means that we remove all terms that occur in more than 10% of the training documents. This can be a useful way to remove common terms that might not be captured by a standard stopword list, and it has the same basic motivation: we want to remove terms that are not really informative. Setting `min_df=.0025` has the effect of removing terms that occur in less than 0.25% of documents, so it gets rid of rare terms which - although they might be informative - don't show up in the corpus enough times for our model to really \"learn\" anything about them. Fiddling with both of these parameters may change your results for better or worse, so it may be worth experimenting with different values here if you're not satisfied with the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0261531-319d-441a-a290-ea8040aa91c4",
   "metadata": {},
   "source": [
    "Now we'll use `fit` to train the pipeline object with our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b917eb8-4f3a-4ad2-ae33-e483da3d413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133c2c1-effc-4cf8-815f-1b25e8d9d121",
   "metadata": {},
   "source": [
    "Now, using `nb_pipeline.predict()` to a list of strings will automatically process the text, apply the model, and return a predicted class. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a63c1f-3ff0-48d3-9bf5-6f1238ed84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Best movie I've ever seen\",\n",
    "             \"This one is a total flop\",\n",
    "             \"I give this one two thumbs up.\",\n",
    "             \"I almost walked out of the theatre.\"\n",
    "            ]\n",
    "\n",
    "# 1 = predicted positive, 0 = predicted negative\n",
    "nb_pipeline.predict(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514a845-ddf4-4fcd-bb39-f7370c1fbffd",
   "metadata": {},
   "source": [
    "Apply this to our testing data and then evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f468a-e1cc-40ad-ad91-bbdaf6804f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f854554-f7f1-4d4c-9b0e-ce857eeac340",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, preds,  margins=True).rename_axis(index = 'Truth', columns='Predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ade76-4f4f-4e25-b7bc-6095e47ebdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034d882-53d0-4b98-a005-380022f3dd1e",
   "metadata": {},
   "source": [
    "Here's a quick reminder for interpreting these metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcab4ae-ce2e-4934-9184-0c13b0983a7a",
   "metadata": {},
   "source": [
    "| **metric**                  | Description                                                                                                                               |\n",
    "|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **accuracy** | % of predictions that are accurate                                                                                                        |\n",
    "| **recall**                  | % of actually positive reviews that were correctly classified as positive                                                                 |\n",
    "| **precision**               | % of predicted positive reviews that were actually positive                                                                               |\n",
    "| **f-1**                     | Harmonic mean of precision and recall. Used as an overall measure of model performance. The maximum score is 1. Scores above .5 are poor. |\n",
    "| **Cohen's Kappa**           | Measures how well the model performs relative to a model based on the marginal probabilities of each class. Higher is better.             |\n",
    "| **Balanced Accuracy**       | Accuracy score after accounting for imbalance between each class                                                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afd3c1-0e38-4375-a543-3f07dd9ebe3a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;font-weight: bold;\">Question 2:</h2><p style=\"color:red;font-weight: bold;\">Run the code below to get predictions from the VADER sentiment analysis tool on the testing data only. Then create a classification report that compares the results from VADER to the actual values of y_test</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a18d0-4e47-4b56-82f2-50e5b2cfc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "vader_scores = [sia.polarity_scores(i)['compound'] for i in X_test]\n",
    "vader_preds = [i>=.05 for i in vader_scores]\n",
    "\n",
    "# now compare vader_preds to y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4154ec-a730-45ab-aa50-668f052c0499",
   "metadata": {},
   "source": [
    "So how does the naive bayes model perform relative to VADER? More importantly why is there a difference? Primarily, this probably comes down to the differences in context: there are a lot of terms that indicate negative views in the IMDB corpus that probably wouldn't indicate negative views in other contexts. We can get a sense of this by extracting some of the most important features from the model using the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba8685-50f0-41e2-ad48-56e10a3390e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(labels, features, nbmodel):\n",
    "    \"\"\"Takes a set of labels, feature names, and a fitted naive bayes classifier and returns the odds ratio of p(positive|word)/p(negative|word)\n",
    "    Code adapted from: https://stackoverflow.com/a/62175164\"\"\"\n",
    "    # get the probability of positive and negative classes\n",
    "    prob_neg = labels.value_counts(normalize=True)[0]\n",
    "    prob_pos = labels.value_counts(normalize=True)[1]\n",
    "    # making a data frame with the results\n",
    "    df_nbf = pd.DataFrame()\n",
    "    df_nbf.index = features\n",
    "    vals= np.e**(nbmodel.feature_log_prob_[0, :])\n",
    "    # np.e exponentiates the logged odds, so this turns them back into probabilities \n",
    "    df_nbf['pos'] = np.e**(nbmodel.feature_log_prob_[1, :]) # log probability for negative class\n",
    "    df_nbf['neg'] = np.e**(nbmodel.feature_log_prob_[0, :]) # log probability for positive class\n",
    "    # terms with the highest ratio of association with predicting one class\n",
    "    # p(positive|word)/p(negative|word) * (p(positive)/p(negative))\n",
    "    df_nbf['odds_positive'] = (df_nbf['pos']/df_nbf['neg'])*(prob_pos /prob_neg)\n",
    "    df_nbf = df_nbf.sort_values('odds_positive',ascending=False).reset_index(names='term')\n",
    "    return df_nbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c2775-2b90-42ba-bca3-7ab58eba81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = nb_pipeline.named_steps['DTM'].get_feature_names_out() # getting the terms from the dtm\n",
    "nb = nb = nb_pipeline.named_steps['naivebayes']                   # getting the weights from the classifier\n",
    "\n",
    "getFeatures(y_train, features, nb).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ee653-fb94-42d3-868a-5d9fb812fff0",
   "metadata": {},
   "source": [
    "Now we can look at some features associated with positive or negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3fc62-7f51-4b25-a43a-81f5e8f2b432",
   "metadata": {},
   "source": [
    "And with a little reshaping, we can plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2167e9-a218-47d4-b537-4cb5424cbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = getFeatures(y_train, features, nb)\n",
    "top_bottom = pd.concat([top.iloc[:15], top.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'term',    \n",
    "                 hue='term',\n",
    "                x=np.log(top_bottom['odds_positive']),dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Strength of association with positive\\n vs. negative reviews', ylabel='term')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d5c72-457d-4cf4-aeb7-cf0227de976c",
   "metadata": {},
   "source": [
    "The results here should give you a rough idea of how and why the Naive Bayes model is able to outperform VADER: there are a number of terms - particularly the names of actors (Seagal, Matthau) and directors (Boll) - that are strongly associated with negative or positive reviews in this corpus. \n",
    "\n",
    "\n",
    "<b>Note adaptability can be a double edged sword:</b> the model makes no distinction between *terms* that indicate negative feelings and *specific groups or individuals* who might be frequent targets of negative sentiments in the training data. As a result, its easy to fit models that make inferences that reflect biases - including cultural, racial, or gender biases - that we don't want to perpetuate. There's no easy fix for this problem, so it's important to remain cognizant of this risk, especially when using machine learning models to make high stakes decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13607b83-2109-4826-88a7-6919c96d2d1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Improving the pre-processing\n",
    "\n",
    "Our pre-processing steps can make a big difference in model performance. I'll try a new processing step that includes **bi-grams** and that reweights the data so that certain terms count for more using the **TF-IDF** scheme. You can click the boxes below to get a little more detail on how these modify our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d31930-42e3-4fd9-99ba-5a1d398125c9",
   "metadata": {},
   "source": [
    "\n",
    "### Bi-grams\n",
    "<details>\n",
    "  <summary>(Click to expand)</summary>\n",
    "N-grams are consecutive sequences of words. If I take a sentence like \"See spot run\"\n",
    "\n",
    "- **uni-grams** are (see, spot, run)\n",
    "- **bi-grams** are (see_spot, spot_run)\n",
    "- **tri-gram** is (see_spot_run) \n",
    "\n",
    "Adjusting the `ngram_range=(0,1)` to `ngram_range=(0,2)` will make a document term matrix that includes all unigrams AND all bigrams in the corpus. Note that this will have the effect of nearly doubling the number of columns in my matrix, so the model will take noticeably longer to run, but we may get some improved predictions as a result.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d19573-50a5-4050-854e-8e5473c8a831",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TF-IDF\n",
    "<details>\n",
    "  <summary>(Click to expand)</summary>\n",
    "\n",
    "\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) weighting a common weighting causes rare words to recieve a bigger weight in the term-document matrix. The default version of the scheme used by scikit learn uses this formula:\n",
    "\n",
    "$$\\text{TF-IDF} = \\text{TF}(t,d) \\times \\text{IDF}(t, D) $$\n",
    "\n",
    "Where: \n",
    "$$\\text{TF}(t,d) = \\frac{\\text{Number of occurrences of term } t \\text{ in document } d}{\\text{Total number of words in document } d}$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\text{IDF}(t) = \\log  \\dfrac{\\text{Total number of documents} +1 }{\\text{Number of documents containing term } t + 1} +1 $$\n",
    "\n",
    "\n",
    "So, the \"TF\" part simply converts the word counts to proportions, while the \"IDF\" weighting is inversely proportional to how common each term is. So a common term would have a lower weight and rare terms will have higher weights.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45cfa3-df8d-47f5-8ec8-221ff58152bd",
   "metadata": {},
   "source": [
    "Here's our new pipeline function. Note that all I've really changed here is the vectorizer function and the `ngram_range` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79bb965e-98db-4f09-9b83-b8174ff08e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a pipeline\n",
    "nb_tfidf_ngram = Pipeline([\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,2), # groups of multiple words\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )),\n",
    "    ('naivebayes',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e38d81-cffa-4455-9abc-02f30fb83651",
   "metadata": {},
   "source": [
    "Now we can apply it to the data and fit a new model to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d88afb-b3b6-4ebc-88a6-a29913f74a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tfidf_ngram.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be0bbe-0442-40a4-ab1c-557ab7e0ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb_tfidf_ngram.predict(X_test)\n",
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                           target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d75299-099f-4cda-bc8d-0adfd8891ffb",
   "metadata": {},
   "source": [
    "Now compare the important features from this model to the important features from the model that used unweighted unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe0c1e-bdf1-494d-ad65-90ef75408583",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = nb_tfidf_ngram.named_steps['tfidf_vectorizer'].get_feature_names_out() # getting the terms from the dtm\n",
    "nb_model = nb_tfidf_ngram.named_steps['naivebayes']                   # getting the weights from the classifier\n",
    "\n",
    "top = getFeatures(y_train, features, nb_model)\n",
    "top_bottom = pd.concat([top.iloc[:15], top.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'term',    \n",
    "                 hue='term',\n",
    "                x=np.log(top_bottom['odds_positive']),dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Strength of association with positive\\n vs. negative reviews', ylabel='term')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a95ca6-0107-407f-a736-089074175375",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;font-weight: bold;\">Question 3:</h2> \n",
    "<p style=\"color:red;font-weight: bold;\">Try changing the pre-processing steps for the reviews data. Consider making changes to <code>max_df</code> or <code>min_df</code>, or use a different stemming algorithm.  What steps you choose here is up to you. Fit a new naive bayes model and compare your results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7185f48-1c6b-4204-84e4-280c14d91857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f9786-01ae-4c76-adeb-f15d84c65d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fitting a different model\n",
    "\n",
    "Can we do better with a better model? The Naive Bayes classifier, after all, fails to account for correlations between predictors. Logistic regression is one alternative that does not have the same \"naive\" assumptions. While a standard logit model wouldn't quite work here (because we have more predictors than observations) the scikit learn version of the logit model uses a lasso penalty that performs a kind of automatic variable selection during the fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1587d2d1-9202-4efd-926a-153978f9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create a pipeline\n",
    "logistic_tfidf_ngram = Pipeline([\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,2), # groups of multiple words\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )),\n",
    "    ('logistic',LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "logit_model = logistic_tfidf_ngram.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50444d92-7f66-40a3-8426-417fff588bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.82      0.85       499\n",
      "    positive       0.84      0.90      0.87       501\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.86      0.86      0.86      1000\n",
      "weighted avg       0.86      0.86      0.86      1000\n",
      "\n",
      "cohens kappa:  0.7199585538659722\n",
      "balanced accuracy:  0.8599274397097589\n"
     ]
    }
   ],
   "source": [
    "preds = logit_model.predict(X_test)\n",
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62e2f6-c91c-4e5d-94a5-c02b1c635632",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "\n",
    "Although its unlikely, given how many test examples we have, we might want to make sure that our performance with this model isn't just a function of random chance, and we also want to avoid over-fitting to a single test data set. To avoid this problem, we'll often use k-fold cross validation. In K-fold cross validation, we separate the data into \"K\" equally sized groups, and then loop through the folds using each one as the validation data and all of the remaining observations as training data. Scikit-learn has an easy method for running k-fold cross validation and getting some overall metrics. I'll create a new pipeline for this, but this is just to make sure I get a model that hasn't \"seen\" any of the reviews data before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9084cf16-6ce0-4641-8ff2-9d30f8a3bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(analyzer = 'word',\n",
    "                             ngram_range=(0,1), \n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )),\n",
    "    ('naivebayes',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "cross_val = cross_validate(cross_validation_pipeline, \n",
    "               reviews.text, \n",
    "               reviews.label, \n",
    "               cv=40,\n",
    "               scoring =['f1', 'balanced_accuracy']\n",
    "              \n",
    "              )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8347844-4054-44cf-b782-825c28c10f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.400579</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.846313</td>\n",
       "      <td>0.846393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011234</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.032429</td>\n",
       "      <td>0.031659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.395329</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.784050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.396755</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.824405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.398012</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>0.847961</td>\n",
       "      <td>0.847798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.399468</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.872024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.465295</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.911802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fit_time  score_time    test_f1  test_balanced_accuracy\n",
       "count  40.000000   40.000000  40.000000               40.000000\n",
       "mean    0.400579    0.011481   0.846313                0.846393\n",
       "std     0.011234    0.000906   0.032429                0.031659\n",
       "min     0.395329    0.010026   0.784000                0.784050\n",
       "25%     0.396755    0.010760   0.823292                0.824405\n",
       "50%     0.398012    0.011383   0.847961                0.847798\n",
       "75%     0.399468    0.012152   0.873016                0.872024\n",
       "max     0.465295    0.013395   0.909091                0.911802"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_val).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

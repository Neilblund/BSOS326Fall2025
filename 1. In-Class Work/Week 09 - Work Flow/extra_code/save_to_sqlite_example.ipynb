{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149abb4d-9b95-4c79-929d-db294302ec5a",
   "metadata": {},
   "source": [
    "# Saving to a SQL data base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54edca-ba88-48ce-8c6b-54d2112fd8bd",
   "metadata": {},
   "source": [
    "This is an example of using a SQL database to store results from webscraping or API queries. The primary advantage of setting things up this way is that SQL databases are good for handling large structured databases and you don't need to hold much data in memory at any one given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7342807-6987-4f7c-a131-2380588b2307",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe461b5a-ba2e-462b-8b78-fcd9b4131cfa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ap_parser(url):\n",
    "    site = get(url)\n",
    "    content = BeautifulSoup(site.content, \"html.parser\")\n",
    "    timestamp = int(content.select_one('.Page-content bsp-timestamp').get('data-timestamp'))\n",
    "    output = {\n",
    "        'url' : site.url,\n",
    "        'tags' : ', '.join([i.get('content') for i in content.select('meta[property=\"article:tag\"]')]),\n",
    "        'section' : ' '.join([i.get('content') for i in content.select('meta[property=\"article:section\"]')]),\n",
    "        'authors': ', '.join([i.get_text() for i in content.select('.Page-authors .Link')]),\n",
    "        'article_text' : ' '.join([i.get_text() for i in content.select('.Page-content .RichTextBody p')]),\n",
    "        'pubdate' : time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp/1000)), # the timestamp on AP articles is a unix epoch\n",
    "        'headline' : ' '.join([i.get_text() for i in content.select('.Page-headline')])\n",
    "    }\n",
    "    return output\n",
    "\n",
    "# check if a table already exists, if it doesn't then create one\n",
    "conn = sqlite3.connect('./ap_articles.db')\n",
    "\n",
    "cur = conn.cursor()\n",
    "print(\"checking for table\")\n",
    "try: \n",
    "    cur.execute('SELECT * from articles limit 1')\n",
    "    print(\"table exists\")\n",
    "except sqlite3.OperationalError:\n",
    "    print(\"No such table: creating\")\n",
    "    cur.execute('''\n",
    "                CREATE TABLE articles(\n",
    "                url text,\n",
    "                tags text,\n",
    "                section text,\n",
    "                authors text,\n",
    "                article_text text,\n",
    "                pubdate text,\n",
    "                headline text       \n",
    "                )''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6018df-b9e0-4c28-8398-5acac127e171",
   "metadata": {},
   "source": [
    "From here, I'm just doing something similar to what we did in classwork 9-1: grabbing a sitemap, getting a list of articles and scraping each one. The key difference is that my results go into a SQL database called `ap_articles.db` and I only bother visiting a URL if there's not already a row for it in the data base. This means that I can come back over and over again to add new articles as they come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4036fd30-85a4-4e67-b4d8-d70e244f36a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sitemap = get('https://apnews.com/ap-sitemap.xml')\n",
    "\n",
    "overall_sitemap = BeautifulSoup(sitemap.content, features=\"xml\") \n",
    "# select all <loc> nodes\n",
    "overall_nodes = overall_sitemap.select('loc')\n",
    "# loop through the entire list and just get the link\n",
    "sitemap_urls = [i.get_text() for i in overall_nodes]\n",
    "sitemaps = [i for i in sitemap_urls if bool(re.search('2024|2025', i)) ]\n",
    "\n",
    "article_urls = []\n",
    "\n",
    "for i in sitemaps:\n",
    "# get the sitemap\n",
    "    response = get(i)\n",
    "    # parse the content as an XML document\n",
    "    sitemap= BeautifulSoup(response.content, features=\"xml\") # note the features = 'xml' option!\n",
    "    # select all <loc> nodes\n",
    "    url_nodes = sitemap.select('loc')\n",
    "    urls = [i.get_text() for i in url_nodes]\n",
    "    article_urls.extend([i for i in urls if bool(re.search(\"/article/\", i)) ])\n",
    "    time.sleep(.1)\n",
    "len(article_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c37d8d-0795-4eb8-80ff-4cc3895ad700",
   "metadata": {},
   "source": [
    "Searching for inflation related articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06863e18-c909-4a96-b3e0-0c9373cdb48c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_list = [i for i in article_urls if bool(re.search(r\"inflation\", i)) ]\n",
    "len(articles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6f3fb-ffce-45bf-8992-a35248acc398",
   "metadata": {},
   "source": [
    "If new ones exist, then add them, otherwise, skip: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae68ec7-b58a-4803-a14d-e88f52b7ee89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for url in articles_list:\n",
    "    article_count = pd.read_sql(f\"SELECT COUNT(*) as article_count from articles  where url = '{url}'\", con=conn)\n",
    "    if article_count['article_count'][0]==0:\n",
    "        print(\"not found, scraping article\", end='\\r')\n",
    "        try:\n",
    "            article = ap_parser(url)\n",
    "        except: \n",
    "            print(\"error encountered\")\n",
    "        finally:\n",
    "            pd.DataFrame(article, index=[0]).to_sql(con = conn, name='articles', if_exists='append', index=False)\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print('article found', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122c794-062f-4d68-80ba-35cfb399e09f",
   "metadata": {},
   "source": [
    "How many articles do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc704b8d-833d-46ab-a25b-1ac59be65f88",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(*)\n",
       "0       210"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql(\"SELECT COUNT(*) from articles\", con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de55aca-d519-4421-bbcc-ce21d4e143be",
   "metadata": {},
   "source": [
    "When you're done, close the connection to the SQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db2569d2-71ed-4b74-b924-163de80dc1ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

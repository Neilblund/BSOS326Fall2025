{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0660a349-b3ef-4082-b7a9-b05c5048461d",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98587fd-260d-4fa8-8b69-1d57efcb3eb3",
   "metadata": {},
   "source": [
    "You may need to install `nltk`, if you haven't done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c574a-d708-4ab1-b5b1-ab2d0dd90476",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf830732-d580-4896-b93d-9d16cc06f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer, LancasterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# included so I can add latex code\n",
    "from IPython.display import display, Math, Latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81de662-0be5-437f-b01f-27b988af33b0",
   "metadata": {},
   "source": [
    "You'll also need to download some additional functions and data by running this code. (you'll generally only need to do this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307105e-7b18-41f6-aeab-17c2a561c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('popular')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c0b3b-a306-4feb-b3aa-ec80cb2d467f",
   "metadata": {},
   "source": [
    "Importing the IMDB review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33054313-2776-4a32-aded-ff92b2c2ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7a3e8-2983-4a31-9e6e-b669276468ed",
   "metadata": {},
   "source": [
    "When we left last class, we were using a set of user reviews from IMDB to check the accuracy of the VADER sentiment analysis lexicon. We found that, overall, it did better than a random guess, but still had plenty of errors. But can we do better? \n",
    "\n",
    "Maybe! Instead of using a lexicon to determine if things are positive or negative, we can make a machine learning model that infers the terms that are associated with positive or negative documents automatically. In general, even fairly simple machine learning models can beat lexicon based methods. \n",
    "\n",
    "In **supervised machine learning**, we are focused on finding the relationship between a **label y** and **features x**.\n",
    "\n",
    "$$ y = f(x) $$\n",
    "\n",
    "\"Learning\" is finding a function f that minimizes future error in recovering y. \n",
    "\n",
    "For supervised learning, we must have a y variable that we know. That is, we need to have a the y variable in our dataset, so that we can build our model and use that model to predict y for future data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> in machine learning circles its common to use terms - like \"learning\", \"training\", \"memorization\" etc. - that can give the misleading impression that we're dealing with models that have some kind of human-like reasoning capacity. This is a useful metaphor because the idea of \"learning\" is a lot more intuitive than \"minimizing a loss function\", but what we're really doing here is fitting statistical models. Some of these models are very sophisticated, but none are conscious, they don't think, they don't have free will, and this is true even for the most cutting edge technologies. Just something to keep in mind. \n",
    "</div>\n",
    "\n",
    "\n",
    "We're going to compare the predictions from the VADER sentiment lexicon to another widely used machine learning model called a Naive Bayes classifier. \n",
    "\n",
    "\n",
    "The Naive Bayes model is *old* and extremely simple by modern standards, but its performs surprisingly well for simple classification tasks and is often used as a sort of baseline model for assessing other machine classifiers. It works by using Baye's Theorem to calculate the probability of each class (C_k in the formula below) given the predictors (X). \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/52bd0ca5938da89d7f9bf388dc7edcbd546c118e)\n",
    "\n",
    "It's considered \"naive\" because it assumes (incorrectly!) that each word is independent of all other words. This is another one of those clearly wrong assumptions that still works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8e720-de78-48ac-bfd8-754e2e55098a",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Our goal here is to apply a mathematical model to text data, so a basic step is going to be converting words into some kind of a sensible numeric format.\n",
    "\n",
    "In the last class, we used VADER, which does a lot of the \"words-into-numbers\" stuff for us. But for the current case we're going to need to do that process ourselves. As a starting point, we'll use what's commonly called a \"bag-of-words\" representation of our texts. A \"bag-of-words\" model simplifies texts by ignoring things like word-ordering, parts of speech, tone, context etc. and instead just focuses on \"how many times a term occurs in a given document\". To convert our reviews to a bag-of-words, we're going to do the following pre-processing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb601e-20d6-4062-9250-91bfddaf2c17",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Tokenization** splits texts into smaller units. In the current case, this will be individual words without punctuation. But it could also be sentences, paragraphs or \"n-gram\" (groups of 1, 2, 3... words)\n",
    "2. **Stopword Removal** removing common terms like \"a\", \"and\", \"the\" that are grammatically useful but uninformative for many text models\n",
    "3. **Normalization** combining terms that are more-or-less equivalent by doing things like converting words to lower-case or removing word endings through stemming or lemmatization\n",
    "\n",
    "\n",
    "(In some cases we may change the ordering of these steps, or we might do some cleaning and normalization and then decide, based on a closer look at our results, that we need to go back and do some additional cleaning. Our end goal, however, is generally to a representation of a text that is simple *enough* without sacrificing too much nuance or complexity.\n",
    "\n",
    "Once we've made our bag-of-words, we'll use it to create what is called a \"document-term-matrix\" where each row represents a single document, each column represents a word that occurs anywhere in the entire collection of documents, and the cell values indicate a count of the number of times word `j` occurs in document `i`\n",
    "\n",
    "For example, if I think about the sentences as a collection of three documents:\n",
    "\n",
    ">    \"See Spot Run. Spot runs fast. Run spot run!\"\n",
    "\n",
    "\n",
    "I would represent those documents as a document-term-matrix that looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af7739-32c2-453c-852d-147448e1a075",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\"><thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\">See</th>\n",
    "    <th class=\"tg-0pky\">Spot</th>\n",
    "    <th class=\"tg-0pky\">Run</th>\n",
    "    <th class=\"tg-0pky\">Fast</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">See Spot run.</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Spot runs fast</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Run Spot run.</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Note that, for real-world text analysis, the number of columns is going to grow exponentially as we include more documents, and most cells are just going to have values of `0`. This tendency to get very large \"sparse\" matrices is a recurring problem in text analysis and we'll have to make a lot of simplifications to keep things manageable. Many of the processing steps below are intended to allow us to combine or drop columns from our document term matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afcbbf-a5c7-40a1-bf2a-bda8da44607d",
   "metadata": {},
   "source": [
    "### Making the document-term-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508fed80-f7c9-4b4f-880f-8bee18e3ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104328e6-5f6a-4f5b-83c9-5da08834e083",
   "metadata": {},
   "source": [
    "The `scikit-learn` package has a `CountVectorizer` function that will let us do all of the necessary processing in one step, but just to get a sense of what each component does, we'll try each part separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f07b14-57eb-4edf-9fd6-01ee279abb50",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "\n",
    "In the tokenization step, we'll split up documents into individual words. To do this, we'll use the `word_tokenize` function from the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab076900-6b3b-4298-91e7-24b8438e3ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = 'See Spot run. Spot runs fast. Run Spot run!'\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f8055-7464-4e3e-a08b-38c6a5dc2f58",
   "metadata": {},
   "source": [
    "For the IMDB reviews, we're going to go a step further and lower case all of the texts. This will ensure that our document-term-matrix treats \"This\", \"this\", and \"THIS\" as a single word instead of creating separate columns for terms that mean the same thing, so our output will end up looking something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eac810-6a1c-491d-89f1-aab727214f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lower case and drop empty\n",
    "\n",
    "nltk.word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719f82a-5893-4f57-a2fc-6e783caf5f64",
   "metadata": {},
   "source": [
    "### 2. Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17a111-88dc-476f-a5e4-4ba8b8c8e90b",
   "metadata": {},
   "source": [
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with the `nltk` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea25547-2f8a-42d2-b6c7-404f7fe3f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb965b4-2b93-4256-aa92-f7065e40dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcd57b-035a-45f0-b296-f2c0555ae596",
   "metadata": {},
   "source": [
    "We'll convert this list of stopwords to a set (this will speed up some processing steps), and then we'll use it to filter the results from the tokenization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cde93a-cd38-41d8-8389-1036fd5dc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the stopword list and converting it to a list\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This text contains a couple of stopwords.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "filtered_tokens = [w for w in tokens if w not in  eng_stopwords]\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde506f3-5b58-468b-8162-521341568b04",
   "metadata": {},
   "source": [
    "While we're filtering, we might also want to remove punctuation marks from the bag of words. So we'll use `.isalpha()` to remove anything that isn't an a letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe8c25-3863-4f59-98c7-232d60c3a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [w for w in tokens if w not in  eng_stopwords and w.isalpha()]\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648dd45-7185-4698-83ae-a52ff94371d5",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00124386-92f4-46f0-88e6-09477c8dcbe1",
   "metadata": {},
   "source": [
    "Finally, we'll try to simplify our bag-of-words analysis by grouping together different inflections of the same terms. Word-endings that indicate things like pluralization and tense are useful in the context of human communication, but they're not informative when we're trying to do things like identify the topic or tone of a text.\n",
    "\n",
    "There are two common approaches to this kind of normalization: \n",
    "\n",
    "- **Lemmatization** uses parts-of-speech and context clues to convert words to their basic dictionary form. \n",
    "- **Stemming** uses some simple hueristics to remove word inflections. \n",
    "\n",
    "Stemming is more error-prone than lemmatization, and sometimes results in words that you won't find in a dictionary, but it has the advantage of being much faster because it relies in simple rules whereas lemmatization considers word context and parts-of-speech. \n",
    "\n",
    "There are multiple stemming algorithms with different rule sets and differing strengths and weaknesses. In this notebook, we'll use the Snowball Stemmer. You'll notice this works pretty well for many words, but gives odd results for others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741d976-42d3-4b7d-8184-a32773edd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stemming algorthim\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d3c6c-0f96-447a-9f19-5a5a4a77b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it to some terms\n",
    "forms = ['lying', 'fisherman', 'change', 'systematic', 'stapled', 'catlike', 'argument', 'alphabetical']\n",
    "print([stemmer.stem(i) for i in forms])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ece9f-a3f8-4cbd-80d4-c3cd57646d6d",
   "metadata": {},
   "source": [
    "You can contrast these results with the results from an alternate stemming algorithm, such as the LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fccf4-b1f1-4190-94b6-589cf407116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcb7fd-576b-4205-989d-7ece20d57b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lying', 'fisherman', 'change', 'systematic', 'stapled', 'catlike', 'argument', 'alphabetical']\n",
    "print([lancaster.stem(i) for i in forms])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07bbc1-297a-4fbe-9579-b8fec17e58f7",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We'll wrap all three steps together in a single function that will take a document and return a processed bag of words. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db623e6c-30d0-4c13-b85c-86ac8bfc014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens if token not in eng_stopwords and token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bbc49-eb1c-47cd-9abb-7431b2e719cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"This is a sentence that has been fully processed into a bag of words!! Isn't it great?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699a9d1-4d87-45c0-99d6-b24f72556713",
   "metadata": {},
   "source": [
    "## Using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed0511-751e-4d90-83f2-f3d356671a21",
   "metadata": {},
   "source": [
    "Now, we'll apply our tokenizer to the IMBD reviews and create the function that will output our term-document-matrix.\n",
    "\n",
    "In addition to stemming and stopword removal, you might notice we also set values for `max_df` and `min_df`. `max_df=0.1` means that we remove all terms that occur in more than 10% of the documents. This can be a useful way to remove common terms that might not be captured by a standard stopword list, and it has the same basic motivation: we want to remove terms that are not really informative. Setting `min_df=.0025` has the effect of removing terms that occur in less than 0.25% of documents, so it gets rid of rare terms which - although they might be informative - don't show up in the corpus enough times for our topic model to really \"learn\" anything about them. Fiddling with both of these parameters may change your results for better or worse, so it may be worth experimenting with different values here if you're not satisfied with the results of your topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa415e-4041-421b-bc63-a17a79343c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens if token not in eng_stopwords and token.isalpha()]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acb92d-e9e6-46b1-8d81-a8a90bc93f7a",
   "metadata": {},
   "source": [
    "Just to get a sense of how this looks, we'll apply it to the first 10 documents and then convert it back into a Pandas Dataframe. Note that you generally won't want/need to do this, its just a way to see what the function is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bca36e-c848-4067-a6d5-5bbc6144a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(reviews.text[:10])\n",
    "pd.DataFrame.sparse.from_spmatrix(dtm).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837722a-febd-4770-ade9-df38eb125128",
   "metadata": {},
   "source": [
    "The columns here have numbers instead of names, but we can figure out which word is represented by each column by examining the `feature_names` from the vectorizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2f7ed-1519-4cf3-bd8f-4946d229f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69284f27-3728-4eb1-822f-291901e5323d",
   "metadata": {},
   "source": [
    "No we've got everything we need to process the data for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a7c27-a0a9-40c6-84df-6ef9e488a53c",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "One more caveat before we start: its extremely easy to make a model that \"predict\" data that's already part of the sample. A list of all U.S. presidential election winners in sequential order is a perfect predictor of all past elections, but its a terrible model for predicting the next one. Creating a model that predicts well inside a sample but poorly outside the sample is called \"overfitting\", and its a recurring problem for this kind of machine learning. In order to avoid that problem, we're going to randomly split our data into a training set that we'll feed to the model, and a validation (or training) set, that we'll use to assess our accuracy. \n",
    "\n",
    "The `train_test_split` function will do this for us. We'll use 80% of our data for training and the remaining 20% for validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ee57e-6a8c-4298-a755-fe07272d1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews.text, reviews.label,\n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bd1d9-dc65-4a44-a434-4ced4c029121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a training set and testing set:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241b26a-5870-4158-ad79-af4864819543",
   "metadata": {},
   "source": [
    "Now we can apply the the vectorization function to make our bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d04036-2d72-4d1a-b564-0f46a2eb7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# get the names of the features for future use\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Note! We use transform instead of fit_transform to ensure that the vectorization function\n",
    "# doesn't update anyhing based on the testing data\n",
    "test_dtm = vectorizer.transform(X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ba108-77ce-40b4-9918-69ca5a66be7c",
   "metadata": {},
   "source": [
    "Now we'll fit our Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c5180-8b96-44ea-a10f-6f1ae01d5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724e9ba-75b4-455a-b243-3cfcb9f1b34e",
   "metadata": {},
   "source": [
    "Now we can use the trained model to predict data in the testing set, and compare the results in a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33be22b-542e-49d9-a0cd-a9e853a653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.predict(test_dtm)\n",
    "\n",
    "pd.crosstab(y_test, preds,  margins=True).rename_axis(index = 'Truth', columns='Predictions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044c58-6c9e-4c55-814f-4fd793513034",
   "metadata": {},
   "source": [
    "`scikit-learn` has a built in function that will allow us to get some additional information about our model performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba32873-150b-42bd-b47b-eb7b04e58347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034d882-53d0-4b98-a005-380022f3dd1e",
   "metadata": {},
   "source": [
    "Here's how we can intepret these metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcab4ae-ce2e-4934-9184-0c13b0983a7a",
   "metadata": {},
   "source": [
    "| **metric**                  | Description                                                                                                                               |\n",
    "|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **accuracy** | % of predictions that are accurate                                                                                                        |\n",
    "| **recall**                  | % of actually positive reviews that were correctly classified as positive                                                                 |\n",
    "| **precision**               | % of predicted positive reviews that were actually positive                                                                               |\n",
    "| **f-1**                     | Harmonic mean of precision and recall. Used as an overall measure of model performance. The maximum score is 1. Scores above .5 are poor. |\n",
    "| **Cohen's Kappa**           | Measures how well the model performs relative to a model based on the marginal probabilities of each class. Higher is better.             |\n",
    "| **Balanced Accuracy**       | Accuracy score after accounting for imbalance between each class                                                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afd3c1-0e38-4375-a543-3f07dd9ebe3a",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red;font-weight: bold;\">Question 1: Run the code below to get predictions from the VADER sentiment analysis tool. Then create a classification report that compares the results from VADER to the actual values of y_test</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a18d0-4e47-4b56-82f2-50e5b2cfc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "vader_scores = [sia.polarity_scores(i)['compound'] for i in X_test]\n",
    "vader_preds = [i>=.05 for i in vader_scores]\n",
    "\n",
    "# now compare vader_preds to y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414b7f4-574c-441d-b439-cc28c422eba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df4154ec-a730-45ab-aa50-668f052c0499",
   "metadata": {},
   "source": [
    "So how does the naive bayes model perform relative to VADER? More importantly why is there a difference? Primarily, this probably comes down to the differences in context: there are a lot of terms that indicate negative views in the IMDB corpus that probably wouldn't indicate negative views in other contexts. We can get a sense of this by extracting some of the most important features from the model using the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba8685-50f0-41e2-ad48-56e10a3390e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(labels, features, nbmodel):\n",
    "    # get the probability of positive and negative classes\n",
    "    prob_neg = labels.value_counts(normalize=True)[0]\n",
    "    prob_pos = labels.value_counts(normalize=True)[1]\n",
    "    # making a data frame with the results\n",
    "    df_nbf = pd.DataFrame()\n",
    "    df_nbf.index = features\n",
    "    vals= np.e**(nbmodel.feature_log_prob_[0, :])\n",
    "    # np.e exponentiates the logged odds, so this turns them back into probabilities \n",
    "    df_nbf['pos'] = np.e**(nbmodel.feature_log_prob_[1, :]) # log probability for negative class\n",
    "    df_nbf['neg'] = np.e**(nbmodel.feature_log_prob_[0, :]) # log probability for positive class\n",
    "    # terms with the highest ratio of association with predicting one class\n",
    "    # p(positive|word)/p(negative|word) * (p(positive)/p(negative))\n",
    "    df_nbf['odds_positive'] = (df_nbf['pos']/df_nbf['neg'])*(prob_pos /prob_neg)\n",
    "    df_nbf = df_nbf.sort_values('odds_positive',ascending=False).reset_index(names='term')\n",
    "    return df_nbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c2775-2b90-42ba-bca3-7ab58eba81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeatures(y_train, features, nb).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ee653-fb94-42d3-868a-5d9fb812fff0",
   "metadata": {},
   "source": [
    "Now we can look at some features associated with positive or negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3fc62-7f51-4b25-a43a-81f5e8f2b432",
   "metadata": {},
   "source": [
    "And with a little reshaping, we can plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2167e9-a218-47d4-b537-4cb5424cbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = getFeatures(y_train, features, nb)\n",
    "top_bottom = pd.concat([top.iloc[:15], top.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'term',    \n",
    "                 hue='term',\n",
    "                x=np.log(top_bottom['odds_positive']),dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Strength of association with positive\\n vs. negative reviews', ylabel='term')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d5c72-457d-4cf4-aeb7-cf0227de976c",
   "metadata": {},
   "source": [
    "The results here should give you a rough idea of how and why the Naive Bayes model is able to outperform VADER: there are a number of terms - particularly the names of actors and directors - that are strongly associated with negative or positive reviews in this corpus that wouldn't really be seen as indicating negativity elsewhere. It might also give you a sense of one of the key risks of using machine learning approaches: its easy to unintentionally fit models that make inferences that might reflect biases - including cultural, racial, or gender biases - that we don't want to perpetuate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13607b83-2109-4826-88a7-6919c96d2d1c",
   "metadata": {},
   "source": [
    "# Improving the processing\n",
    "\n",
    "Naive Bayes is generally considered a baseline model for classification, but sometimes we can improve our results with some additional pre-processing.\n",
    "\n",
    "I'll try a new processing step that includes \"N-grams\" and that reweights the data so that certain terms count for more using the TF-IDF scheme.\n",
    "\n",
    "`N-grams` are essentially sequences of words. If I take a sentence like \"See Spot run\", the UNI-grams are just: \"See\", \"Spot\", and \"run\". However, I could also include \"Bi-grams\" or \"Tri-grams\" in my bag of words. The list of bigrams would be: \"See_Spot\" and \"Spot_run\". Adjusting the `ngram_range` from `(0,1)` to `(0,2)` argument would create a document-term-matrix that included all single words and also all bi-grams in the collection of texts. This can improve the performance of our classifier, because it allows us to include a bit more context about each word. \n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a common weighting scheme for texts that causes rare words to recieve a bigger weight in the term-document matrix. Essentially, if a term only occurs in 10% of documents, I'll re-weight each occurence so that they count 10x as much. The effect will be that rare words are treated as more important in our model, while common words are treated as less important. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45cfa3-df8d-47f5-8ec8-221ff58152bd",
   "metadata": {},
   "source": [
    "Here's our new text pre-processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f5cbf-e637-45af-98aa-84b18dda5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb965e-98db-4f09-9b83-b8174ff08e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens if token not in eng_stopwords and token.isalpha()]\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,2), # groups of multiple words\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e38d81-cffa-4455-9abc-02f30fb83651",
   "metadata": {},
   "source": [
    "Now we can apply it to the data and fit a new model to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d88afb-b3b6-4ebc-88a6-a29913f74a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtm = tfidf_vectorizer.fit_transform(X_train)\n",
    "features = tfidf_vectorizer.get_feature_names_out()\n",
    "test_dtm = tfidf_vectorizer.transform(X_test) \n",
    "\n",
    "# fit the model\n",
    "tfidf_nb = MultinomialNB()\n",
    "tfidf_nb.fit(train_dtm, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be0bbe-0442-40a4-ab1c-557ab7e0ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tfidf_nb.predict(test_dtm)\n",
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                           target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d75299-099f-4cda-bc8d-0adfd8891ffb",
   "metadata": {},
   "source": [
    "Now compare the important features from this model to the important features from the model that used unweighted unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe0c1e-bdf1-494d-ad65-90ef75408583",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = getFeatures(y_train, features, tfidf_nb)\n",
    "top_bottom = pd.concat([top.iloc[:15], top.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'term',    \n",
    "                 hue='term',\n",
    "                x=np.log(top_bottom['odds_positive']),dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Strength of association with positive\\n vs. negative reviews', ylabel='term')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a95ca6-0107-407f-a736-089074175375",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red;font-weight: bold;\">Question 2: Try changing the pre-processing steps for the reviews data. Consider making changes to `max_df` or `min_df`, or use a different stemming algorithm.  What steps you choose here is up to you. Fit a new naive bayes model and compare your results.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7185f48-1c6b-4204-84e4-280c14d91857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f9786-01ae-4c76-adeb-f15d84c65d15",
   "metadata": {},
   "source": [
    "## Fitting a different model\n",
    "\n",
    "Can we do better with a better model? The Naive Bayes classifier, after all, fails to account for correlations between predictors and can't really handle interactive relationships. Support Vector Machines are an alternative machine learning algorithm that can account for these kinds of differences. So does it produce a better predictor?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587d2d1-9202-4efd-926a-153978f9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "model.fit(train_dtm,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50444d92-7f66-40a3-8426-417fff588bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dtm)\n",
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd05a3d-3558-43d2-a819-ff42e1bcb8f8",
   "metadata": {},
   "source": [
    "# Extra stuff (if we get to it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9f579-7368-4345-b918-533cb443b65f",
   "metadata": {},
   "source": [
    "## Making a pipeline\n",
    "\n",
    "I've got a lot of steps here that I need to apply repeatedly to the data. `scikitlearn` provides a `Pipeline` class that will allow us to combine several processing and prediction steps into a single function so that we can concisely apply a model in a bunch of different contexts. I'll create a pipeline that performs the text processing and applying a Naive Bayes model in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5791cae-afd8-4671-8234-14da61efd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(analyzer = 'word',\n",
    "                             ngram_range=(0,1), \n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )),\n",
    "    ('naivebayes',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8026c25-b76b-4de4-b8a3-60ffef52c40a",
   "metadata": {},
   "source": [
    "Calling the pipeline will give me a summary of the steps involved in using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31308096-1007-48ca-ad51-dc97ec6670b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca32ac-5b5f-474d-9066-0c60f4f32a96",
   "metadata": {},
   "source": [
    "Now, I can apply it to some training data to create a trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8027753-c9de-41c4-b433-3483cf6f1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline.fit(reviews.text, reviews.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f006190-231c-48be-b325-188f341fdebd",
   "metadata": {},
   "source": [
    "And now I have a trained model that can take a list of entirely new texts and classify them as either positive (1) or negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019697a-1191-46fa-bb1c-3da40c741ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [\"This movie was great\", \n",
    "               \"This is my favorite movie\", \n",
    "               \"I hated this movie!\", \n",
    "               \"Worst movie ever\"]\n",
    "\n",
    "print(pipeline.predict(new_reviews))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62e2f6-c91c-4e5d-94a5-c02b1c635632",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "\n",
    "Although its unlikely, given how many test examples we have, we might want to make sure that our performance with this model isn't just a function of random chance, and we also want to avoid over-fitting to a single test data set. To avoid this problem, we'll often use k-fold cross validation. In K-fold cross validation, we separate the data into \"K\" equally sized groups, and then loop through the folds using each one as the validation data and all of the remaining observations as training data. Scikit-learn has an easy method for running k-fold cross validation and getting some overall metrics. I'll create a new pipeline for this, but this is just to make sure I get a model that hasn't \"seen\" any of the reviews data before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9084cf16-6ce0-4641-8ff2-9d30f8a3bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(analyzer = 'word',\n",
    "                             ngram_range=(0,1), \n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )),\n",
    "    ('naivebayes',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "cross_val = cross_validate(cross_validation_pipeline, \n",
    "               reviews.text, \n",
    "               reviews.label, \n",
    "               cv=40,\n",
    "               scoring =['f1', 'balanced_accuracy']\n",
    "              \n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06c3d50f-c597-40b9-9f4e-edb72d06c3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.580208</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.846313</td>\n",
       "      <td>0.846393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.044545</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.032429</td>\n",
       "      <td>0.031659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.525702</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.784050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.552001</td>\n",
       "      <td>0.015006</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.824405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.575709</td>\n",
       "      <td>0.016002</td>\n",
       "      <td>0.847961</td>\n",
       "      <td>0.847798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.589537</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.872024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.755628</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.911802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fit_time  score_time    test_f1  test_balanced_accuracy\n",
       "count  40.000000   40.000000  40.000000               40.000000\n",
       "mean    0.580208    0.016656   0.846313                0.846393\n",
       "std     0.044545    0.002935   0.032429                0.031659\n",
       "min     0.525702    0.013000   0.784000                0.784050\n",
       "25%     0.552001    0.015006   0.823292                0.824405\n",
       "50%     0.575709    0.016002   0.847961                0.847798\n",
       "75%     0.589537    0.017250   0.873016                0.872024\n",
       "max     0.755628    0.032006   0.909091                0.911802"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_val).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

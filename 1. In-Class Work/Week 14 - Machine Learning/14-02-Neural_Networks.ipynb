{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0e313e-63d3-44fd-a6eb-9878d3c531cb",
   "metadata": {},
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcaf40-0058-4ef7-8b17-081607b0fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03f68d-df00-409a-b279-18f2be18afb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0d448-d499-48bc-aa49-58292512674f",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "\n",
    "In this notebook, we will go over how to train neural network models to do supervised machine learning using tensorflow. This is mostly to demonstrate how the code works and how one goes about building these types of models. A more detailed explanation of what exactly is going on behind the scenes and the math behind the implementation is reserved for another class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d6128-089a-4031-bb5e-d079c7b9da7f",
   "metadata": {},
   "source": [
    "### Prediction Example\n",
    "\n",
    "Let's take a look at a quick example of doing some prediction. The `ncbirths` dataset has information on births in North Carolina, including information about the mother, weeks of pregnancy, and whether the baby was a low birthweight baby or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250ec37-605e-4622-8757-65bfbaa2c3b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncbirths = pd.read_csv(\"ncbirths.csv\").dropna()\n",
    "ncbirths.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08070f88-2801-45b9-acf8-c5bd0e8c428b",
   "metadata": {},
   "source": [
    "To make some of our later assessments easier, we're going to \"dummify\" some of the categorical variables. This generate K-1 columns (1 for each category) each with a zero if that observation is a member of that category, or a 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58691eb-2479-44c5-b2a3-3f4a9719373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummified = pd.get_dummies(ncbirths, columns=['lowbirthweight','racemom','habit','gender'],  \n",
    "                            drop_first=True, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e2cd4-4d3d-49d8-af45-6df0a7614c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f10f1-9e28-47b7-a283-2f0d0f46d361",
   "metadata": {},
   "source": [
    "We'll try a very simple example of predicting the low birthweight status of the baby using the number of weeks that the pregnancy lasted. If we were to take a look at the relationship with a graph, it might look like the following. Note that `1` refers to low birthweight while `0` refers to not low birthweight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c620c92-b811-4e35-a86d-cad3fe3a5e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummified.plot.scatter(y = 'lowbirthweight_not low', x = 'weeks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce645c4-839d-45b1-828b-45b97aa1d7bf",
   "metadata": {},
   "source": [
    "So, how do we use `weeks` to predict the low birthweight status? Well, using a straight line to show the relationship wouldn't make sense. This is because low birthweight can only take one of two values: `0` or `1`. So, instead, we try to create a curved function that is constrained between 0 and 1 and represents the **probability** of low birth weight at different values of \"weeks\".\n",
    "\n",
    "To do this, we'll model the relationship using a logistic or sigmoid function, which follows an S-shaped curve that is constrained between 0 and 1. You can see this function by running the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac323182-f69e-46ff-b5b3-dd45da34c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10, 10, 500)\n",
    "x = tf.cast(x, tf.float32)\n",
    "f = lambda x : (1/20)*x + 0.6\n",
    "plt.plot(x, tf.math.sigmoid(x))\n",
    "plt.ylim((-0.1,1.1))\n",
    "plt.title(\"Sigmoid function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe474e1e-f3df-4f10-84d2-64f08763a528",
   "metadata": {},
   "source": [
    "In a logistic regession model, we attempt to find the parameters of this function that be most likely to have generated the observed data. \n",
    "\n",
    "## Running the model\n",
    "\n",
    "Just like we did in the previous class, we'll try to avoid overfitting our data by splitting up the data into a training set and an evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433324ed-b07a-4174-a0de-65aba4953773",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data =dummified['lowbirthweight_not low'].tolist()\n",
    "\n",
    "x_data = dummified.drop(columns=['lowbirthweight_not low'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data,  y_data,\n",
    "                                     test_size=0.20, # % of observations for validation\n",
    "                                     random_state = 500\n",
    "                                    ) # this is a random process, so you want to set a random seed! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579407f-3ffb-41b9-957d-560cbf89caec",
   "metadata": {},
   "source": [
    "Let's take a look at what happens when we fit a logistic regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e82a1-e4c8-4caf-961d-abaca2ed8c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_cols =['weeks']\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(x_train[pred_cols], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78678d-0b0b-4567-80ad-a53fbc2a5ea7",
   "metadata": {},
   "source": [
    "We can see how the model is operating by looking at some predictions at different values of \"weeks\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f071f5-a9ab-497e-9090-c35a230b7c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_by_week = pd.DataFrame({'weeks':x_test.weeks.sort_values().unique()})\n",
    "pred_by_week['preds'] =logit.predict_proba(pred_by_week)[:,1]\n",
    "fig, axes = plt.subplots(figsize=(8,6))\n",
    "pred_by_week.plot.line('weeks','preds', ax = axes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2523b-f51e-4ada-8023-280b5a44546d",
   "metadata": {},
   "source": [
    "Now, we just want to use the model to predict our held-out data and then report the results. We'll use the same set of metrics we used in the previous class. First, we'll look at the confusion matrix to compare predictions to the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387300b-9e6e-481c-a4f0-e050ae2e798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logit.predict_proba(x_test[pred_cols])[:,1]\n",
    "\n",
    "\n",
    "pd.crosstab(y_test, preds>=.5,  margins=True).rename_axis(index = 'Truth', columns='Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0bfed-3581-4969-be68-1fa40354f235",
   "metadata": {},
   "source": [
    "And then we'll look at some summary statistics for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f432f8-6f18-460e-a8f8-8fc9e39c6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds>=.5, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['Low Birth Weight', 'Not Low Birthweight']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds>=.5))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds>=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21494349-c07e-4369-b673-b689c7c6eacb",
   "metadata": {},
   "source": [
    "The predictions here are ok overall. Still, it looks like it might do well for some, but there are lots of points that it does poorly on. That's probably because we're just using one feature (variable) to make a prediction. In reality, we might want to use many features, and we might suspect that they have a complicated set of relationships with each other. Their effect on birthweight might be curvilinear, or be different depending on the prescence or absence of some other characteristic.\n",
    "\n",
    "That's where something like neural networks might come in. The above logistic regression is an example of what one **node** in a neural network might look like. Neural networks essentially work by combining lots of these types of simple relationships to create a complex model that makes predictions. The key advantage of a neural network is that a sufficiently complex network can approximate **any functional relationship** between the predictors and the outcome that you could specify. See [here](http://neuralnetworksanddeeplearning.com/chap4.html) for a mostly visual introduction to this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22702b0e-f5d0-44d0-ac6a-20e4bcdd34ac",
   "metadata": {},
   "source": [
    "![Neural Network](neural_network.png)\n",
    "\n",
    "*Source: https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95dfedd-6576-4a2d-84bc-1a9a25f274aa",
   "metadata": {},
   "source": [
    "To make our neural network, we'll start by converting our data into a format that tensorflow can use. We'll use the same set of predictor variables here that we used in the previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e418f-b10b-42b1-a83e-24a2e9b9f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['weeks','gender_male', 'habit_smoker']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b82472-7c10-4ee4-b686-c25ceef7065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor=tf.convert_to_tensor(x_train[cols])\n",
    "x_test_tensor=tf.convert_to_tensor(x_test[cols])\n",
    "y_train_tensor=tf.convert_to_tensor(y_train)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8139aaa-6c96-4424-9163-a38d7c2de07f",
   "metadata": {},
   "source": [
    "Next, we'll create a normalization layer that will rescale our variables by subtracting their mean and then dividing them by their standard deviation. Neural networks have a tendency to overfit, especially on unbalanced data, and this kind of normalization is one way we can reduce that tendency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d1ed3-4ce3-492a-8526-4602f5eb766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(x_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ead18-543c-477b-9f92-e9f493c3eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out what this does on the first few rows of data:\n",
    "normalizer(x_train_tensor[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607ea1e-6499-404c-a5c4-faba4c340909",
   "metadata": {},
   "source": [
    "Now, we'll build a neural network by adding together several layers in a sequence. The `normalizer` layer just takes our input data and normalizes it. The `Dense` layers are \"nodes\": each one takes our input data and shifts and reweights it using an activation function. The final layer `Dense(1, activation='sigmoid')` is the output layer that converts the outputs from the nodes into a prediction that follows the same functional form as the logistic regression we performed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0e7a0-0480-4e6c-a162-5f2b6117b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_tensor.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "  normalizer,\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ee971-0f50-47cb-8a3c-1786141cc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be3690-e895-4fda-b9af-335cf31ef04c",
   "metadata": {},
   "source": [
    "Now we'll build our model and train it for 10 epochs. Since our classes are imbalanced, we'll also use a `class_weight` to make the the low birth weight cases get more consideration when training the model. This usually helps the network make better predictions when there is an imbalance between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ff7f6-5a95-4133-81d7-dbda8701abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "history =  model.fit(x_train_tensor, y_train_tensor, \n",
    "                     # the number of iterations. This may need to go higher! Especially for compelx models\n",
    "                     epochs=10,\n",
    "                     validation_data = (x_test_tensor, y_test_tensor),\n",
    "                     # controls the amount of output that is printed\n",
    "                     verbose=1,\n",
    "                     # adding a small weighting function.\n",
    "                     class_weight =   {0:5., 1:1.}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e908921-f7e1-47c6-a2c3-e359626efb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_tensor,  y_test_tensor, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca47c4c-100a-4750-b427-f0961112062e",
   "metadata": {},
   "source": [
    "Now, we'll get our predictions and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeeab4a-347e-47e9-81d1-f5ed91e6bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c7b0b-28d4-4369-b8f1-f02bc20a972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test_tensor, preds.flatten()>=.5,  margins=True).rename_axis(index = 'Truth', columns='Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dc88e-8f24-4d9c-b9e5-68d1fcc289c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_tensor, preds>=.5, \n",
    "                            # add target_names to show labels in the report:\n",
    "                              target_names=['Low Birth Weight', 'Not Low Birthweight']))\n",
    "\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test_tensor, preds>=.5))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test_tensor, preds>=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a44073-b0be-4f12-b011-4a5449fbfa41",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;font-weight:bold\">Question</h2>\n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">See if you can improve on the accuracy of the model above. Try adding another \"layer\", increasing the number of nodes, or adding additional features.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abd0ed-8913-4adf-8b77-a537acfdfb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b099a86c-570e-43c8-adec-f99e656b928b",
   "metadata": {},
   "source": [
    "Is this better or worse than the prior model? Any thoughts as to why it might perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e99c9b-0e09-4ca9-9aee-93f0c3c0c9d3",
   "metadata": {},
   "source": [
    "### MNIST Data\n",
    "\n",
    "Let's look at an example of applying neural network modeling to some image data. The MNIST dataset that comes with the `tensorflow` package contains images from handwritten digits (numbers). Our goal is to train a neural network that is able to accurately determine what number is written based on the data from the image. In other words, we want to build a neural network that is able to recognize numbers that have been handwritten. \n",
    "\n",
    "The data itself is structured so that it is in a 2-dimensional format for each observation. Each observation is 28 by 28, with the values within each cell representing the intensity of the pixel. These values make up the **features**, or variables that we use to predict/classify the observation as one of the 10 numerical digits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f108ce-ddf3-40eb-8bd1-c1853cef45b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scale it so that the values are between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04941a4e-4508-4148-8727-cb4ac5cfc235",
   "metadata": {},
   "source": [
    "To visually see what the data look like, let's graph some of the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613e2d7-57fc-4654-960f-af341f572c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9926a6f-66b2-4278-bc6b-a278e80b3279",
   "metadata": {},
   "source": [
    "We specify the neural network model using `Sequential` and adding the layers in a list. \n",
    "\n",
    "Let's take a look at the layers one by one.\n",
    "- `Flatten(input_shape=(28, 28))`: This flattens the 28 by 28 data into a 1-D format. There isn't anything being done to values at this step -- all that is happening is that the 2-D shape is being changed to a 1-D shape so that all of the same values are in a vector format.\n",
    "- `Dense(128, activation='relu')` / `Dense(64, activation='relu')` : This is a dense layer, with the first argument specifying how many nodes there are. We have two dense layers in this neural network: one with 128 nodes and one with 64 nodes. You can imagine all of the features (variables) in our data feeding into every single one of the 128 nodes, and the outputs of those 128 nodes feeding into the 64 nodes in the next step. \n",
    "- `Dense(10)`: This is an **output layer**. Since we are trying to predict the image as being one of ten different categories (that is, the individual digits values from 0 to 9), we need a layer with 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dc701-f016-4b2f-9bd1-17399a6cacc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Flatten(input_shape=(28, 28)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047a661-1eea-4381-b52c-52798217539e",
   "metadata": {},
   "source": [
    "Then, we need to compile the model, specify the loss function, and give it the metric we will use to evaluate how it is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a446ec-790f-42dc-8ffa-1224fda478e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b91f06-9c4e-4686-949a-f6e6dbc31292",
   "metadata": {},
   "source": [
    "Finally, we fit the model by giving it our data. Since we are using the training set to build our model, we give it the x and y data from the train. We also set the batch size and the number of epochs to 10. The **batch size** refers to how much of the data is used to fit the model at a time. An **epoch** refers to the number of times that the full data has been sent through the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f8240-03b7-4f41-a00e-22ab297710d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size = 32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d880a-2c8e-44f7-8ecc-4b5b7d74247a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now, let's take a look at how this would do on new data. We can use the `evaluate` method to apply our trained model to the test set and see how accurate it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9972438-8b26-4316-9dab-6d00c3595d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5439a3-bdf2-4c7e-93be-1c875f17a5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    if np.argmax(predictions[i]) == y_test[i]:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.xlabel(f'Predicted: {np.argmax(predictions[i])}, Actual: {y_test[i]}', color = color)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
